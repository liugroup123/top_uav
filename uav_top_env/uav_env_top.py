import numpy as np
import gym
from gym import spaces
import pygame
import os
from gym.utils import seeding
import torch
from torch_geometric.data import Data
from gat_model_top import UAVAttentionNetwork, create_adjacency_matrices
from config import ExperimentConfig, create_config, config_manager

class UAVEnv(gym.Env):
    def __init__(
        self,
        config=None,  # é…ç½®å¯¹è±¡æˆ–é…ç½®åç§°
        config_file=None,  # é…ç½®æ–‡ä»¶è·¯å¾„
        # ä»¥ä¸‹å‚æ•°ç”¨äºå‘åå…¼å®¹ï¼Œå¦‚æœæä¾›configåˆ™å¿½ç•¥
        num_agents=None,
        num_targets=None,
        world_size=None,
        coverage_radius=None,
        communication_radius=None,
        max_steps=None,
        render_mode=None,
        screen_size=None,
        render_fps=None,
        dt=None,
        experiment_type=None,
        topology_change_interval=None,
        topology_change_probability=None,
        min_active_agents=None,
        max_active_agents=None,
        initial_active_ratio=None
    ):
        super().__init__()

        # å¤„ç†é…ç½®
        self.config = self._load_config(
            config, config_file,
            num_agents, num_targets, world_size, coverage_radius,
            communication_radius, max_steps, render_mode, screen_size,
            render_fps, dt, experiment_type, topology_change_interval,
            topology_change_probability, min_active_agents, max_active_agents,
            initial_active_ratio
        )

        # ä»é…ç½®ä¸­è®¾ç½®ç¯å¢ƒå‚æ•°
        env_config = self.config.environment
        self.num_agents = env_config.num_agents
        self.num_targets = env_config.num_targets
        self.world_size = env_config.world_size
        self.coverage_radius = env_config.coverage_radius
        self.communication_radius = env_config.communication_radius
        self.max_steps = env_config.max_steps
        self.dt = env_config.dt
        self.curr_step = 0
        self.max_coverage_rate = 0.0

        # ä»ç‰©ç†é…ç½®è®¾ç½®å‚æ•°
        physics_config = self.config.physics
        self.max_accel = physics_config.max_accel
        self.max_speed = physics_config.max_speed

        # å®éªŒç±»å‹é…ç½®
        topology_config = self.config.topology
        self.experiment_type = topology_config.experiment_type
        self._validate_experiment_type()

        # ä¿å­˜æ‹“æ‰‘å‚æ•°
        self.custom_topology_params = {
            'change_interval': topology_config.topology_change_interval,
            'change_probability': topology_config.topology_change_probability,
            'min_agents': topology_config.min_active_agents,
            'max_agents': topology_config.max_active_agents,
            'initial_active_ratio': topology_config.initial_active_ratio
        }

        # æ‹“æ‰‘å˜åŒ–é…ç½®
        self.topology_config = self._init_topology_config()

        # æ¸²æŸ“é…ç½®
        env_config = self.config.environment
        self.render_mode = env_config.render_mode
        self.width, self.height = env_config.screen_size
        self.screen = None
        self.clock = None
        self.metadata = {"render_fps": env_config.render_fps}

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # GATåˆå§‹åŒ–ï¼ˆä¼ å…¥deviceå‚æ•°ï¼‰
        self.gat_network = UAVAttentionNetwork(
            uav_features=4,  # ä½ç½®(2) + é€Ÿåº¦(2)
            target_features=2,  # ç›®æ ‡ä½ç½®(2)
            hidden_size=64,
            heads=4,
            dropout=0.6,
            device=self.device  # æ·»åŠ deviceå‚æ•°
    )
        # å°†ç½‘ç»œç§»åˆ°æŒ‡å®šè®¾å¤‡
        self.gat_network = self.gat_network.to(self.device)

        # ç‰©ç†å‚æ•°é…ç½®
        self.physics_params = {
            'max_accel': 3.0,
            'max_speed': 5.0,
            'boundary_decay_zone': 0.2,
            'min_speed_ratio': 0.2,
            'bounce_energy_loss': 0.7
        }
        
        # åŠ¨ä½œç©ºé—´é…ç½®
        self.action_scale = 1.0  # åŠ¨ä½œç¼©æ”¾å› å­
        self.action_space = [
            spaces.Box(
                low=-self.action_scale,
                high=self.action_scale,
                shape=(2,),
                dtype=np.float32
            ) for _ in range(self.num_agents)
        ]
        # è§‚å¯Ÿç©ºé—´
        obs_dim = (
            4 +                     # åŸºç¡€çŠ¶æ€ (ä½ç½®+é€Ÿåº¦)
            2 * self.num_targets + # ç›®æ ‡ç›¸å¯¹ä½ç½®
            2 * (self.num_agents - 1) + # é‚»å±…ä¿¡æ¯
            32 +                    # GATç‰¹å¾
            3                       # æ‹“æ‰‘ä¿¡æ¯
        )
        self.observation_space = [spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(obs_dim,), 
            dtype=np.float32
        ) for _ in range(self.num_agents)]

        # æ·»åŠ å¥–åŠ±å‡½æ•°é…ç½®å‚æ•°
        self.reward_params = {
            # è¦†ç›–ç‡ç›¸å…³
            'coverage_weight': 3.5,          # ä»35.0é™åˆ°3.5
            'coverage_exp': 1.5,             # ä¿æŒä¸å˜
            'distance_scale': 1.5,           # ä»15.0é™åˆ°1.5
            
            # è¿é€šæ€§ç›¸å…³
            'connectivity_weight': 2.0,      # ä»20.0é™åˆ°2.0
            'min_connectivity_ratio': 0.3,    # ä¿æŒä¸å˜
            
            # ç¨³å®šæ€§ç›¸å…³
            'stability_weight': 1.5,         # ä»15.0é™åˆ°1.5
            'stability_time_window': 10,      # ä¿æŒä¸å˜
            'stability_threshold': 0.9,       # ä¿æŒä¸å˜
            
            # èƒ½é‡æ•ˆç‡ç›¸å…³
            'energy_weight': 0.5,            # ä»5.0é™åˆ°0.5
            'smoothness_weight': 0.5,        # ä»5.0é™åˆ°0.5
            
            # è¾¹ç•Œæƒ©ç½šç›¸å…³
            'boundary_weight': 1.0,          # ä»10.0é™åˆ°1.0
            'safe_distance': 0.1             # ä¿æŒä¸å˜
        }
        
        # æ·»åŠ å†å²è®°å½•ç”¨äºè®¡ç®—ç¨³å®šæ€§
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # æ ¹æ®å®éªŒç±»å‹åˆå§‹åŒ–UAVçŠ¶æ€
        self._init_uav_states()
        
        # æ·»åŠ æ‹“æ‰‘å˜åŒ–ç›¸å…³çš„å¥–åŠ±å‚æ•°
        self.reward_params.update({
            'reorganization_weight': 2.0,    # é‡ç»„è¡Œä¸ºçš„å¥–åŠ±æƒé‡
            'task_reassign_weight': 1.5,     # ä»»åŠ¡é‡åˆ†é…çš„å¥–åŠ±æƒé‡
            'formation_weight': 1.0,         # é˜Ÿå½¢ç»´æŒçš„å¥–åŠ±æƒé‡
        })
        
        # æ‹“æ‰‘å˜åŒ–çŠ¶æ€
        self.topology_change = {
            'in_progress': False,            # æ˜¯å¦æ­£åœ¨å‘ç”Ÿæ‹“æ‰‘å˜åŒ–
            'change_type': None,             # 'failure' æˆ– 'addition'
            'affected_agent': None,          # å—å½±å“çš„UAVç´¢å¼•
            'start_step': None,              # å˜åŒ–å¼€å§‹çš„æ­¥æ•°
            'pre_change_coverage': None,     # å˜åŒ–å‰çš„è¦†ç›–ç‡
        }

        # Episodeçº§åˆ«çš„æ‹“æ‰‘å˜åŒ–è®¡åˆ’
        self.episode_plan = {
            'type': 'normal',                # 'normal', 'loss', 'addition'
            'trigger_step': None,            # è§¦å‘å˜åŒ–çš„æ­¥æ•°
            'executed': False                # æ˜¯å¦å·²æ‰§è¡Œ
        }

        self.training = True  # é»˜è®¤ä¸ºè®­ç»ƒæ¨¡å¼
        
        # GATç½‘ç»œåˆå§‹åŒ–æ—¶è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.gat_network.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        
        # ä¼˜åŒ–1ï¼šæ·»åŠ ç¼“å­˜
        self._cache = {}
        
        # ä¼˜åŒ–2ï¼šé¢„åˆ†é…å¸¸ç”¨æ•°ç»„
        self._agent_mask = np.zeros(num_agents, dtype=bool)
        
        # ä¼˜åŒ–3ï¼šè®¾ç½®CUDAä¼˜åŒ–å‚æ•°
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.cuda.empty_cache()

        self.reset()

    def reset(self, seed=None):
        self.curr_step = 0
        self.max_coverage_rate = 0.0

        # åˆå§‹åŒ–éšæœºç§å­
        if seed is not None:
            np.random.seed(seed)

        # åˆå§‹åŒ–æ— äººæœºä½ç½®ï¼ˆç¡®ä¿ä½¿ç”¨float32ï¼‰
        self.agent_pos = []
        bottom_y = -self.world_size + 0.15
        spacing = 2 * self.world_size / (self.num_agents + 1)
        for i in range(self.num_agents):
            x = -self.world_size + (i + 1) * spacing
            self.agent_pos.append([x, bottom_y])
        self.agent_pos = np.array(self.agent_pos, dtype=np.float32)  # æ˜ç¡®æŒ‡å®šfloat32

        # åˆå§‹åŒ–é€Ÿåº¦
        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åˆå§‹åŒ–ç›®æ ‡ç‚¹ä½ç½®
        self.target_pos = np.random.uniform(
            -self.world_size*0.85, 
            self.world_size*0.85, 
            (self.num_targets, 2)
        ).astype(np.float32)  # ç¡®ä¿ä½¿ç”¨float32

        # æ„å»ºåˆå§‹é‚»æ¥çŸ©é˜µ
        self._build_adjacency_matrices()

        # é‡ç½®å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åˆ¶å®šæœ¬episodeçš„æ‹“æ‰‘å˜åŒ–è®¡åˆ’
        self._plan_episode_topology()

        obs_list = self._get_obs()
        return {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}, {}


    def step(self, action_dict):
        """ä¼˜åŒ–çš„æ­¥è¿›å‡½æ•°"""
        # ä¼˜åŒ–1ï¼šé¢„åˆ†é…åŠ¨ä½œæ•°ç»„
        actions = np.zeros((self.num_agents, 2), dtype=np.float32)
        
        # ä¼˜åŒ–2ï¼šæ‰¹é‡å¤„ç†åŠ¨ä½œ
        for i in self.active_agents:
            actions[i] = action_dict[f"agent_{i}"]
        
        # ä¼˜åŒ–3ï¼šæ‰¹é‡æ›´æ–°çŠ¶æ€
        mask = np.array([i in self.active_agents for i in range(self.num_agents)])
        
        # å¤„ç†åŠ¨ä½œå’Œæ›´æ–°çŠ¶æ€
        self.agent_vel[mask] = np.array([
            self._process_action_and_dynamics(actions[i], i)
            for i in self.active_agents
        ])
        
        # æ›´æ–°ä½ç½®
        self.agent_pos[mask] += self.agent_vel[mask] * self.dt
        np.clip(self.agent_pos, -self.world_size, self.world_size, out=self.agent_pos)
        
        # éæ´»è·ƒUAVé€Ÿåº¦ç½®é›¶
        self.agent_vel[~mask] = 0
        
        # æ›´æ–°é‚»æ¥çŸ©é˜µ
        self._build_adjacency_matrices()

        self.curr_step += 1

        # æ ¹æ®å®éªŒç±»å‹è§¦å‘æ‹“æ‰‘å˜åŒ–
        self.trigger_topology_change()

        obs_list = self._get_obs()
        rewards_list = self._compute_rewards()
        dones_list = [self.curr_step >= self.max_steps] * self.num_agents

        return (
            {f"agent_{i}": obs_list[i] for i in range(self.num_agents)},
            {f"agent_{i}": rewards_list[i] for i in range(self.num_agents)},
            {f"agent_{i}": dones_list[i] for i in range(self.num_agents)},
            False,
            {f"agent_{i}": {} for i in range(self.num_agents)}
        )


    """
    è§‚å¯Ÿå‡½æ•°ç›¸å…³çš„å‡½æ•°
    """
    def _get_obs(self):
        """è·å–è§‚å¯Ÿï¼Œå¤„ç†åŠ¨æ€UAVæ•°é‡"""
        obs = []

        # åˆ›å»ºæ´»è·ƒUAVçš„ç´¢å¼•æ˜ å°„è¡¨
        active_idx_map = {agent_idx: i for i, agent_idx in enumerate(self.active_agents)}
        
        # åŸºç¡€ç‰¹å¾ï¼šä½ç½®å’Œé€Ÿåº¦
        active_positions = self.agent_pos[self.active_agents]
        active_velocities = self.agent_vel[self.active_agents]
        
        uav_features = torch.tensor(
            np.concatenate([active_positions, active_velocities], axis=1),
            dtype=torch.float32,
            device=self.device
        )
        target_features = torch.tensor(
            self.target_pos,
            dtype=torch.float32,
            device=self.device
        )

        # è·å–æ´»è·ƒUAVçš„é‚»æ¥çŸ©é˜µ
        active_uav_adj = self._get_active_adj_matrix()
        active_target_adj = self._get_active_target_adj_matrix()

        # GATå‰å‘ä¼ æ’­
        if self.training:
            gat_features = self.gat_network(
                uav_features,
                target_features,
                active_uav_adj,
                active_target_adj
            )
        else:
            with torch.no_grad():
                gat_features = self.gat_network(
                    uav_features,
                    target_features,
                    active_uav_adj,
                    active_target_adj
                )

        gat_features_np = gat_features.detach().cpu().numpy()

        # ä¸ºæ¯ä¸ªUAVç”Ÿæˆè§‚å¯Ÿ
        for i in range(self.num_agents):
            if i in self.active_agents:
                # ä½¿ç”¨æ˜ å°„è¡¨è·å–æ­£ç¡®çš„GATç‰¹å¾ç´¢å¼•
                gat_idx = active_idx_map[i]
                
                obs_i = np.concatenate([
                    self.agent_pos[i],                # ä½ç½® (2)
                    self.agent_vel[i],                # é€Ÿåº¦ (2)
                    self._get_relative_targets(i),    # ç›®æ ‡ç›¸å¯¹ä½ç½® (num_targets*2)
                    self._get_neighbor_info(i),       # é‚»å±…ä¿¡æ¯ (num_agents-1)*2
                    gat_features_np[gat_idx],         # GATç‰¹å¾ (hidden_size//2)
                    self._get_topology_info()         # æ‹“æ‰‘ä¿¡æ¯ (3)
                ])
                obs.append(obs_i.astype(np.float32))
            else:
                # å¯¹äºéæ´»è·ƒUAVï¼Œè¿”å›é›¶å‘é‡
                obs.append(np.zeros(self.observation_space[0].shape, dtype=np.float32))

        return obs

    def _get_relative_targets(self, agent_idx):
        """è·å–ç›¸å¯¹ç›®æ ‡ä½ç½®"""
        return (self.target_pos - self.agent_pos[agent_idx]).reshape(-1).tolist()

    def _get_neighbor_info(self, agent_idx):
        """è·å–é‚»å±…ä¿¡æ¯"""
        neighbor_info = []
        for j in range(self.num_agents):
            if j != agent_idx:
                if j in self.active_agents:
                    delta = self.agent_pos[j] - self.agent_pos[agent_idx]
                    if np.linalg.norm(delta) <= self.communication_radius:
                        neighbor_info.extend(delta)
                    else:
                        neighbor_info.extend([0.0, 0.0])
                else:
                    neighbor_info.extend([0.0, 0.0])
        return neighbor_info

    def _get_topology_info(self):
        """è·å–æ‹“æ‰‘å˜åŒ–ä¿¡æ¯"""
        return [
            1.0 if self.topology_change['in_progress'] else 0.0,
            1.0 if self.topology_change['change_type'] == 'failure' else 0.0,
            float(self.curr_step - self.topology_change['start_step']) / self.max_steps 
            if self.topology_change['in_progress'] else 0.0
        ]
    
    # æ„å»ºé‚»æ¥çŸ©é˜µ
    def _build_adjacency_matrices(self):
        """æ„å»ºå®Œæ•´çš„é‚»æ¥çŸ©é˜µï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        # ç¡®ä¿ä½¿ç”¨float32ç±»å‹
        uav_positions = torch.tensor(self.agent_pos, dtype=torch.float32, device=self.device)
        target_positions = torch.tensor(self.target_pos, dtype=torch.float32, device=self.device)
        
        # æ‰¹é‡è®¡ç®—è·ç¦»
        uav_dists = torch.cdist(uav_positions, uav_positions)
        target_dists = torch.cdist(uav_positions, target_positions)
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        self.uav_adj = (uav_dists <= self.communication_radius).float()
        self.target_adj = (target_dists <= self.coverage_radius).float()
        
        # ç§»é™¤è‡ªç¯
        self.uav_adj.fill_diagonal_(0)
        
        # éæ´»è·ƒUAVçš„è¿æ¥è®¾ä¸º0
        mask = torch.zeros(self.num_agents, dtype=torch.bool, device=self.device)
        for i in self.active_agents:
            mask[i] = True
        
        # å°†éæ´»è·ƒUAVçš„è¿æ¥è®¾ä¸º0
        self.uav_adj[~mask, :] = 0
        self.uav_adj[:, ~mask] = 0
        self.target_adj[~mask, :] = 0

    """
    å¥–åŠ±å‡½æ•°ç›¸å…³çš„å‡½æ•°
    """
    def _compute_coverage_reward(self):
        """è®¡ç®—è¦†ç›–ç‡å¥–åŠ±"""
        coverage_rate, _, _, _ = self.calculate_coverage_complete()
        
        # è®¡ç®—åˆ°ç›®æ ‡çš„å¹³å‡æœ€å°è·ç¦»
        distances = np.linalg.norm(self.agent_pos[:, None, :] - self.target_pos[None, :, :], axis=2)
        avg_min_dist = np.mean(np.min(distances, axis=0))
        normalized_dist = np.clip(avg_min_dist, 0, self.reward_params['distance_scale'])
        
        # éçº¿æ€§è¦†ç›–ç‡å¥–åŠ±
        coverage_score = (coverage_rate ** self.reward_params['coverage_exp']) * normalized_dist
        return coverage_score * self.reward_params['coverage_weight']

    def _compute_connectivity_reward(self):
        """è®¡ç®—è¿é€šæ€§å¥–åŠ±"""
        _, connected, _, unconnected_count = self.calculate_coverage_complete()
        
        # è®¡ç®—è¿é€šç‡
        connectivity_ratio = 1.0 - (unconnected_count / self.num_agents)
        
        # ä½¿ç”¨å¹³æ»‘çš„è¿é€šæ€§å¥–åŠ±
        if connectivity_ratio >= self.reward_params['min_connectivity_ratio']:
            connectivity_reward = self.reward_params['connectivity_weight'] * connectivity_ratio
        else:
            # ä½äºé˜ˆå€¼æ—¶ç»™äºˆè´Ÿå¥–åŠ±
            connectivity_reward = -self.reward_params['connectivity_weight'] * (
                self.reward_params['min_connectivity_ratio'] - connectivity_ratio
            )
            
        return connectivity_reward

    def _compute_stability_reward(self):
        """è®¡ç®—ç¨³å®šæ€§å¥–åŠ±"""
        coverage_rate, connected, _, _ = self.calculate_coverage_complete()
        self.coverage_history.append(coverage_rate)
        
        # ä¿æŒå›ºå®šé•¿åº¦çš„å†å²è®°å½•
        window_size = self.reward_params['stability_time_window']
        if len(self.coverage_history) > window_size:
            self.coverage_history = self.coverage_history[-window_size:]
        
        # è®¡ç®—è¦†ç›–ç‡çš„ç¨³å®šæ€§
        if len(self.coverage_history) >= 2:
            coverage_variance = np.var(self.coverage_history)
            stability_score = np.exp(-coverage_variance * 10)  # ä½¿ç”¨æŒ‡æ•°å‡½æ•°å¹³æ»‘è½¬æ¢
            
            # å½“è¦†ç›–ç‡é«˜ä¸”ç¨³å®šæ—¶ç»™äºˆé¢å¤–å¥–åŠ±
            if (coverage_rate > self.reward_params['stability_threshold'] and 
                stability_score > self.reward_params['stability_threshold'] and 
                connected):
                return self.reward_params['stability_weight'] * stability_score
        return 0.0

    def _compute_energy_efficiency_reward(self, agent_idx, action):
        """è®¡ç®—èƒ½é‡æ•ˆç‡å¥–åŠ±"""
        # è®¡ç®—é€Ÿåº¦å˜åŒ–ï¼ˆåŠ é€Ÿåº¦ï¼‰çš„å¹³æ–¹å’Œ
        velocity_change = np.sum(np.square(self.agent_vel[agent_idx]))
        energy_penalty = -self.reward_params['energy_weight'] * velocity_change * 0.1  # æ·»åŠ é¢å¤–çš„0.1ç¼©æ”¾å› å­
        
        # è®¡ç®—åŠ¨ä½œå¹³æ»‘åº¦
        action_diff = np.sum(np.square(action - self.prev_actions[agent_idx]))
        smoothness_penalty = -self.reward_params['smoothness_weight'] * action_diff * 0.1  # æ·»åŠ é¢å¤–çš„0.1ç¼©æ”¾å› å­
        
        self.prev_actions[agent_idx] = action
        return energy_penalty + smoothness_penalty

    def _compute_boundary_penalty(self, agent_idx):
        """è®¡ç®—è¾¹ç•Œæƒ©ç½š"""
        pos = self.agent_pos[agent_idx]
        safe_distance = self.reward_params['safe_distance']
        
        # è®¡ç®—åˆ°è¾¹ç•Œçš„è·ç¦»
        boundary_distances = self.world_size - np.abs(pos)
        
        # å¦‚æœè·ç¦»å°äºå®‰å…¨è·ç¦»ï¼Œç»™äºˆæƒ©ç½š
        penalties = np.maximum(0, safe_distance - boundary_distances)
        return -self.reward_params['boundary_weight'] * np.sum(penalties) * 0.1  # æ·»åŠ é¢å¤–çš„0.1ç¼©æ”¾å› å­

    def _compute_reorganization_reward(self):
        """è®¡ç®—é‡ç»„å¥–åŠ±"""
        if not self.topology_change['in_progress']:
            return 0.0

        if self.topology_change['change_type'] == 'failure':
            # è®¡ç®—å¤±æ•ˆåçš„è¦†ç›–ç‡æ¢å¤ç¨‹åº¦
            current_coverage = self.calculate_coverage_complete()[0]
            pre_change_coverage = self.topology_change['pre_change_coverage']

            # é¿å…é™¤é›¶é”™è¯¯
            if pre_change_coverage is None or pre_change_coverage <= 0:
                # å¦‚æœä¹‹å‰è¦†ç›–ç‡ä¸º0ï¼Œåˆ™ç›´æ¥ä½¿ç”¨å½“å‰è¦†ç›–ç‡ä½œä¸ºå¥–åŠ±
                recovery_reward = self.reward_params['reorganization_weight'] * current_coverage
            else:
                coverage_recovery = current_coverage / pre_change_coverage
                recovery_reward = self.reward_params['reorganization_weight'] * coverage_recovery

            return recovery_reward

        return 0.0

    def _compute_rewards(self):
        """ä¿®æ”¹åçš„å¥–åŠ±è®¡ç®—ï¼Œåªè€ƒè™‘æ´»è·ƒUAV"""
        coverage_reward = self._compute_coverage_reward()
        connectivity_reward = self._compute_connectivity_reward()
        stability_reward = self._compute_stability_reward()
        reorganization_reward = self._compute_reorganization_reward()
        
        rewards = []
        for i in range(self.num_agents):
            if i in self.active_agents:
                energy_reward = self._compute_energy_efficiency_reward(i, self.prev_actions[i])
                boundary_reward = self._compute_boundary_penalty(i)
                
                total_reward = (
                    coverage_reward +
                    connectivity_reward +
                    stability_reward +
                    energy_reward +
                    boundary_reward +
                    reorganization_reward
                )
            else:
                total_reward = 0.0  # éæ´»è·ƒUAVæ²¡æœ‰å¥–åŠ±
            
            rewards.append(total_reward)
        
        return rewards

    def calculate_coverage_complete(self):
        """ä¿®æ”¹åçš„è¦†ç›–ç‡è®¡ç®—ï¼Œåªè€ƒè™‘æ´»è·ƒUAV"""
        covered_flags = []
        for target in self.target_pos:
            covered = False
            for agent_idx in self.active_agents:  # åªè€ƒè™‘æ´»è·ƒUAV
                distance = np.linalg.norm(target - self.agent_pos[agent_idx])
                if distance <= self.coverage_radius:
                    covered = True
                    break
            covered_flags.append(covered)

        covered_count = sum(covered_flags)
        total_targets = len(self.target_pos)
        coverage_rate = covered_count / total_targets if total_targets > 0 else 0

        # åªè€ƒè™‘æ´»è·ƒUAVçš„è¿é€šæ€§
        active_num = len(self.active_agents)
        adjacency_matrix = np.zeros((active_num, active_num), dtype=bool)
        for i, agent_i in enumerate(self.active_agents):
            for j, agent_j in enumerate(self.active_agents[i + 1:], i + 1):
                distance = np.linalg.norm(self.agent_pos[agent_i] - self.agent_pos[agent_j])
                if distance <= self.communication_radius:
                    adjacency_matrix[i, j] = True
                    adjacency_matrix[j, i] = True

        visited = [False] * active_num
        def dfs(idx):
            visited[idx] = True
            for j, connected in enumerate(adjacency_matrix[idx]):
                if connected and not visited[j]:
                    dfs(j)

        if active_num > 0:  # ç¡®ä¿æœ‰æ´»è·ƒUAV
            dfs(0)
        fully_connected = all(visited)
        unconnected_count = visited.count(False)

        if fully_connected:
            self.max_coverage_rate = max(self.max_coverage_rate, coverage_rate)

        return coverage_rate, fully_connected, self.max_coverage_rate, unconnected_count
    
    """
    renderç›¸å…³çš„å‡½æ•°
    """
    def render(self):
        if self.render_mode is None:
            gym.logger.warn(
                "Calling render without specifying render_mode."
            )
            return
        if self.screen is None:
            pygame.init()
            self.screen = pygame.display.set_mode((self.width, self.height))
            pygame.display.set_caption('UAV Topology')
            self.clock = pygame.time.Clock()
        self.draw()
        if self.render_mode == 'rgb_array':
            data = pygame.surfarray.array3d(self.screen)
            return np.transpose(data, (1, 0, 2))
        elif self.render_mode == 'human':
            pygame.display.flip()
            self.clock.tick(self.metadata['render_fps'])

    def draw(self):
        fixed_cam = self.world_size
        self.screen.fill((255, 255, 255))

        def to_screen(pos):
            x, y = pos
            y = -y  # y è½´ç¿»è½¬
            sx = int((x / fixed_cam) * (self.width / 2) + self.width / 2)
            sy = int((y / fixed_cam) * (self.height / 2) + self.height / 2)
            return sx, sy

        # ç”»ç›®æ ‡ç‚¹ï¼ˆç»¿è‰²ï¼‰
        for tpos in self.target_pos:
            sx, sy = to_screen(tpos)
            pygame.draw.circle(self.screen, (0, 255, 0), (sx, sy), 5)

        # å­˜å‚¨æ— äººæœºå±å¹•ä½ç½®ç”¨äºè¿çº¿
        screen_positions = {}

        # ç»˜åˆ¶æ‰€æœ‰UAV
        for i, apos in enumerate(self.agent_pos):
            sx, sy = to_screen(apos)
            screen_positions[i] = (sx, sy)

            # ç»˜åˆ¶æ¢æµ‹åŠå¾„åœ†åœˆï¼ˆå®çº¿ï¼‰
            coverage_radius_px = int((self.coverage_radius / fixed_cam) * (self.width/2))
            pygame.draw.circle(self.screen, (0, 0, 255), (sx, sy), coverage_radius_px, 1)
            
            # ç»˜åˆ¶é€šä¿¡åŠå¾„åœ†åœˆï¼ˆè“è‰²è™šçº¿ï¼‰
            comm_radius_px = int((self.communication_radius / fixed_cam) * (self.width/2))
            # åˆ›å»ºè™šçº¿æ•ˆæœ
            num_segments = 80
            for seg in range(num_segments):
                if seg % 2 == 0:
                    start_angle = 2 * np.pi * seg / num_segments
                    end_angle = 2 * np.pi * (seg + 1) / num_segments
                    start_pos = (
                        sx + int(comm_radius_px * np.cos(start_angle)),
                        sy + int(comm_radius_px * np.sin(start_angle))
                    )
                    end_pos = (
                        sx + int(comm_radius_px * np.cos(end_angle)),
                        sy + int(comm_radius_px * np.sin(end_angle))
                    )
                    pygame.draw.line(self.screen, (70, 130, 180), start_pos, end_pos, 1)

            # åªæœ‰UAVä¸­å¿ƒç‚¹çš„é¢œè‰²æ ¹æ®çŠ¶æ€æ”¹å˜
            if i in self.active_agents:
                color = (0, 0, 255)  # æ´»è·ƒUAVç”¨è“è‰²
            else:
                color = (128, 128, 128)  # å¤±æ•ˆUAVç”¨ç°è‰²
            
            # ç»˜åˆ¶UAVæœ¬ä½“
            pygame.draw.circle(self.screen, color, (sx, sy), 8)

        # åªåœ¨æ´»è·ƒUAVä¹‹é—´ç»˜åˆ¶çº¢è‰²è¿æ¥çº¿
        for i in range(self.num_agents):
            if i not in self.active_agents:
                continue  # è·³è¿‡éæ´»è·ƒUAV
            
            for j in range(i + 1, self.num_agents):
                if j not in self.active_agents:
                    continue  # è·³è¿‡éæ´»è·ƒUAV
                
                # åªåœ¨æ´»è·ƒUAVä¹‹é—´æ£€æŸ¥è¿æ¥
                dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                if dist <= self.communication_radius:
                    pygame.draw.line(
                        self.screen, 
                        (255, 0, 0),  # çº¢è‰²è¿æ¥çº¿
                        screen_positions[i], 
                        screen_positions[j], 
                        1  # çº¿å®½
                    )

        # æ˜¾ç¤ºå®éªŒç±»å‹ä¿¡æ¯
        font = pygame.font.Font(None, 24)
        experiment_text = f"å®éªŒç±»å‹: {self.experiment_type}"
        text_surface = font.render(experiment_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 10))

        # æ˜¾ç¤ºæ´»è·ƒUAVæ•°é‡
        active_count_text = f"æ´»è·ƒUAV: {len(self.active_agents)}/{self.num_agents}"
        text_surface = font.render(active_count_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 35))

        # æ˜¾ç¤ºæ‹“æ‰‘å˜åŒ–çŠ¶æ€
        if self.topology_change['in_progress']:
            font = pygame.font.Font(None, 36)
            if self.topology_change['change_type'] == 'failure':
                text = f"UAV {self.topology_change['affected_agent']} Failed"
                color = (255, 0, 0)  # çº¢è‰²
            else:
                text = f"New UAV {self.topology_change['affected_agent']} Added"
                color = (0, 255, 0)  # ç»¿è‰²

            text_surface = font.render(text, True, color)
            text_rect = text_surface.get_rect()
            text_rect.topright = (self.width - 10, 10)
            self.screen.blit(text_surface, text_rect)

    def close(self):
        if self.screen is not None:
            pygame.quit()
            self.screen = None
    
    #è¿”å›æ™ºèƒ½ä½“ä»¥åŠåŠ¨ä½œç©ºé—´è§‚å¯Ÿç©ºé—´
    @property
    def agents(self):
        return [f"agent_{i}" for i in range(self.num_agents)]

    def get_observation_space(self, agent_id):
        return self.observation_space[int(agent_id.split("_")[1])]

    def get_action_space(self, agent_id):
        return self.action_space[int(agent_id.split("_")[1])]

    def _process_action_and_dynamics(self, action, agent_idx):
        """
        ç»Ÿä¸€å¤„ç†åŠ¨ä½œå½’ä¸€åŒ–å’Œç‰©ç†çº¦æŸ
        Args:
            action: åŸå§‹åŠ¨ä½œ (-1, 1)èŒƒå›´
            agent_idx: æ™ºèƒ½ä½“ç´¢å¼•
        Returns:
            æ›´æ–°åçš„é€Ÿåº¦
        """
        # 1. åŠ¨ä½œå½’ä¸€åŒ–åˆ°åŠ é€Ÿåº¦ç©ºé—´
        normalized_accel = action * self.max_accel
        
        # 2. æ›´æ–°é€Ÿåº¦
        new_velocity = self.agent_vel[agent_idx] + normalized_accel * self.dt
        
        # 3. é€Ÿåº¦é™åˆ¶
        current_speed = np.linalg.norm(new_velocity)
        if current_speed > self.max_speed:
            new_velocity = new_velocity * (self.max_speed / current_speed)
        
        # 4. è¾¹ç•Œå¤„ç†
        pos = self.agent_pos[agent_idx]
        boundary_distance = self.world_size - np.abs(pos)
        
        # åˆ›å»ºè¾¹ç•Œè¡°å‡ç³»æ•°
        decay_zone = 0.15  # å¼€å§‹å‡é€Ÿçš„è¾¹ç•Œè·ç¦»
        min_speed_ratio = 0.2  # æœ€å°é€Ÿåº¦æ¯”ä¾‹
        
        for i in range(2):  # å¯¹xå’Œyåˆ†åˆ«å¤„ç†
            if boundary_distance[i] < decay_zone:
                # # çº¿æ€§è¡°å‡é€Ÿåº¦
                # decay_ratio = max(
                #     min_speed_ratio,
                #     boundary_distance[i] / decay_zone
                # )
                # new_velocity[i] *= decay_ratio
                
                # å¦‚æœå·²ç»è§¦ç¢°è¾¹ç•Œï¼Œåå‘é€Ÿåº¦
                if abs(pos[i]) >= self.world_size:
                    new_velocity[i] *= -0.85  # åå¼¹æ—¶çš„èƒ½é‡æŸå¤±
        
        return new_velocity

    def fail_uav(self, uav_idx):
        """æ¨¡æ‹ŸUAVå¤±æ•ˆ"""
        if uav_idx in self.active_agents:
            # è®°å½•å¤±æ•ˆçŠ¶æ€å’Œæœ€åä½ç½®
            self.uav_states[uav_idx]['active'] = False
            self.uav_states[uav_idx]['last_position'] = self.agent_pos[uav_idx].copy()
            self.uav_states[uav_idx]['last_velocity'] = np.zeros_like(self.agent_vel[uav_idx])  # å¤±æ•ˆåé€Ÿåº¦ä¸º0
            self.uav_states[uav_idx]['failure_step'] = self.curr_step
            
            # ä»æ´»è·ƒåˆ—è¡¨ä¸­ç§»é™¤
            self.active_agents.remove(uav_idx)
            print(f"UAV {uav_idx} å·²å¤±æ•ˆï¼Œå½“å‰ä½ç½®: {self.uav_states[uav_idx]['last_position']}")
            
            # è®°å½•æ‹“æ‰‘å˜åŒ–çŠ¶æ€
            self.topology_change = {
                'in_progress': True,
                'change_type': 'failure',
                'affected_agent': uav_idx,
                'start_step': self.curr_step,
                'pre_change_coverage': self.calculate_coverage_complete()[0]
            }
            
            return True
        return False

    def add_uav(self, position=None):
        """æ·»åŠ æ–°çš„UAV"""
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéæ´»è·ƒçš„UAVç´¢å¼•
        for i in range(self.num_agents):
            if i not in self.active_agents:
                # è®¾ç½®åˆå§‹ä½ç½®
                if position is None:
                    position = self._get_safe_spawn_position()
                
                # æ¿€æ´»UAV
                self.agent_pos[i] = np.array(position, dtype=np.float32)
                self.agent_vel[i] = np.zeros(2, dtype=np.float32)
                self.uav_states[i]['active'] = True
                self.active_agents.append(i)
                
                # è®°å½•æ‹“æ‰‘å˜åŒ–çŠ¶æ€
                self.topology_change = {
                    'in_progress': True,
                    'change_type': 'addition',
                    'affected_agent': i,
                    'start_step': self.curr_step,
                    'pre_change_coverage': self.calculate_coverage_complete()[0]
                }
                
                return i
        return None

    def _get_safe_spawn_position(self):
        """è·å–å®‰å…¨çš„ç”Ÿæˆä½ç½®"""
        # é»˜è®¤åœ¨è¾¹ç•Œç”Ÿæˆ
        spawn_positions = [
            [-self.world_size, -self.world_size],  # å·¦ä¸‹è§’
            [self.world_size, -self.world_size],   # å³ä¸‹è§’
            [0, -self.world_size]                  # åº•éƒ¨ä¸­é—´
        ]
        
        # é€‰æ‹©è·ç¦»ç°æœ‰UAVæœ€è¿œçš„ä½ç½®
        max_min_dist = -1
        best_pos = spawn_positions[0]
        
        for pos in spawn_positions:
            min_dist = float('inf')
            for idx in self.active_agents:
                dist = np.linalg.norm(self.agent_pos[idx] - pos)
                min_dist = min(min_dist, dist)
            if min_dist > max_min_dist:
                max_min_dist = min_dist
                best_pos = pos
            
        return np.array(best_pos)

    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå¹¶è¿›è¡Œç›¸å…³ä¼˜åŒ–"""
        self.training = True
        self.gat_network.train()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜

    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶è¿›è¡Œç›¸å…³ä¼˜åŒ–"""
        self.training = False
        self.gat_network.eval()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜

    def random_fail_uav(self):
        """éšæœºé€‰æ‹©ä¸€ä¸ªæ´»è·ƒçš„UAVè¿›è¡Œå¤±æ•ˆ"""
        if len(self.active_agents) > 1:  # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªUAV
            fail_idx = np.random.choice(self.active_agents)
            return self.fail_uav(fail_idx)
        return False

    def _get_active_adj_matrix(self):
        """è·å–åªåŒ…å«æ´»è·ƒUAVçš„é‚»æ¥çŸ©é˜µ"""
        active_positions = torch.tensor(
            self.agent_pos[self.active_agents],
            dtype=torch.float32,
            device=self.device
        )
        dists = torch.cdist(active_positions, active_positions)
        adj_matrix = (dists <= self.communication_radius).float()
        adj_matrix.fill_diagonal_(0)
        return adj_matrix

    def _get_active_target_adj_matrix(self):
        """è·å–æ´»è·ƒUAVä¸ç›®æ ‡ä¹‹é—´çš„é‚»æ¥çŸ©é˜µ"""
        active_positions = torch.tensor(
            self.agent_pos[self.active_agents],
            dtype=torch.float32,
            device=self.device
        )
        target_positions = torch.tensor(
            self.target_pos,
            dtype=torch.float32,
            device=self.device
        )
        
        # è®¡ç®—æ´»è·ƒUAVä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»
        dists = torch.cdist(active_positions, target_positions)
        adj_matrix = (dists <= self.coverage_radius).float()
        return adj_matrix

    def _validate_experiment_type(self):
        """éªŒè¯å®éªŒç±»å‹å‚æ•°"""
        valid_types = ['normal', 'probabilistic']
        if self.experiment_type not in valid_types:
            raise ValueError(f"å®éªŒç±»å‹å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {valid_types}, å½“å‰å€¼: {self.experiment_type}")

    def _init_uav_states(self):
        """æ ¹æ®å®éªŒç±»å‹åˆå§‹åŒ–UAVçŠ¶æ€"""
        # åŸºç¡€UAVçŠ¶æ€åˆå§‹åŒ–
        self.uav_states = {
            i: {
                'active': True,           # åˆå§‹éƒ½æ˜¯æ´»è·ƒçš„
                'last_position': None,    # å¤±æ•ˆæ—¶çš„ä½ç½®
                'last_velocity': None,    # å¤±æ•ˆæ—¶çš„é€Ÿåº¦
                'failure_step': None      # å¤±æ•ˆçš„æ—¶é—´æ­¥
            } for i in range(self.num_agents)
        }

        # æ‰€æœ‰æ¨¡å¼éƒ½ä»å…¨éƒ¨UAVæ´»è·ƒå¼€å§‹
        self.active_agents = list(range(self.num_agents))

    def _init_topology_config(self):
        """åˆå§‹åŒ–æ¦‚ç‡é©±åŠ¨çš„æ‹“æ‰‘å˜åŒ–é…ç½®"""
        config = {
            'enabled': self.experiment_type == 'probabilistic',
            'min_agents': 3,  # æœ€å°‘ä¿æŒçš„UAVæ•°é‡
            'max_agents': self.num_agents,  # æœ€å¤šUAVæ•°é‡

            # æ¦‚ç‡è®¾ç½® (æ€»å’Œåº”ä¸º100%)
            'normal_probability': 0.80,    # 80% æ­£å¸¸ï¼Œæ— å˜åŒ–
            'loss_probability': 0.15,      # 15% æŸå¤±UAV
            'addition_probability': 0.05,  # 5% æ·»åŠ UAV
        }

        # åº”ç”¨ç”¨æˆ·è‡ªå®šä¹‰å‚æ•°
        if self.custom_topology_params['min_agents'] is not None:
            config['min_agents'] = max(1, self.custom_topology_params['min_agents'])
        if self.custom_topology_params['max_agents'] is not None:
            config['max_agents'] = min(self.num_agents, self.custom_topology_params['max_agents'])

        return config

    def _plan_episode_topology(self):
        """ä¸ºæœ¬episodeåˆ¶å®šæ‹“æ‰‘å˜åŒ–è®¡åˆ’"""
        if not self.topology_config['enabled']:
            self.episode_plan = {
                'type': 'normal',
                'trigger_step': None,
                'executed': False
            }
            return

        # æ ¹æ®æ¦‚ç‡å†³å®šepisodeç±»å‹
        rand = np.random.random()

        if rand < self.topology_config['normal_probability']:
            # 80% æ¦‚ç‡ï¼šæ•´ä¸ªepisodeæ­£å¸¸
            episode_type = 'normal'
            trigger_step = None
        elif rand < self.topology_config['normal_probability'] + self.topology_config['loss_probability']:
            # 15% æ¦‚ç‡ï¼šepisodeä¸­æŸå¤±UAV
            episode_type = 'loss'
            # åœ¨episodeçš„30%-70%ä¹‹é—´éšæœºé€‰æ‹©è§¦å‘æ—¶é—´
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        else:
            # 5% æ¦‚ç‡ï¼šepisodeä¸­æ·»åŠ UAV
            episode_type = 'addition'
            # åœ¨episodeçš„30%-70%ä¹‹é—´éšæœºé€‰æ‹©è§¦å‘æ—¶é—´
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))

        self.episode_plan = {
            'type': episode_type,
            'trigger_step': trigger_step,
            'executed': False
        }

        print(f"ğŸ“‹ Episodeè®¡åˆ’: {episode_type}" +
              (f" (ç¬¬{trigger_step}æ­¥è§¦å‘)" if trigger_step else ""))

    def trigger_topology_change(self):
        """åŸºäºepisodeè®¡åˆ’çš„æ‹“æ‰‘å˜åŒ–è§¦å‘"""
        if not self.topology_config['enabled']:
            return False

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œè®¡åˆ’ä¸­çš„æ‹“æ‰‘å˜åŒ–
        if (self.episode_plan['type'] != 'normal' and
            not self.episode_plan['executed'] and
            self.curr_step >= self.episode_plan['trigger_step']):

            # æ‰§è¡Œè®¡åˆ’ä¸­çš„å˜åŒ–
            if self.episode_plan['type'] == 'loss':
                success = self._execute_uav_loss()
            elif self.episode_plan['type'] == 'addition':
                success = self._execute_uav_addition()
            else:
                success = False

            if success:
                self.episode_plan['executed'] = True
                print(f"ğŸ¯ æ‰§è¡Œè®¡åˆ’: {self.episode_plan['type']} (ç¬¬{self.curr_step}æ­¥)")

            return success

        return False

    def _execute_uav_loss(self):
        """æ‰§è¡ŒUAVæŸå¤±"""
        if len(self.active_agents) <= self.topology_config['min_agents']:
            return False

        # éšæœºé€‰æ‹©ä¸€ä¸ªUAVå¤±æ•ˆ
        fail_idx = np.random.choice(self.active_agents)
        success = self.fail_uav(fail_idx)

        if success:
            print(f"[å®éªŒæ¨¡å¼: UAVæŸå¤±] Step {self.curr_step}: UAV {fail_idx} å¤±æ•ˆ")

        return success

    def _execute_uav_addition(self):
        """æ‰§è¡ŒUAVæ·»åŠ """
        if len(self.active_agents) >= self.topology_config['max_agents']:
            return False

        # æ·»åŠ æ–°UAV
        new_uav_idx = self.add_uav()

        if new_uav_idx is not None:
            print(f"[å®éªŒæ¨¡å¼: UAVæ·»åŠ ] Step {self.curr_step}: æ·»åŠ æ–°UAV {new_uav_idx}")
            return True

        return False



    def get_experiment_info(self):
        """è·å–å®éªŒä¿¡æ¯"""
        return {
            'experiment_type': self.experiment_type,
            'topology_enabled': self.topology_config['enabled'],
            'active_agents_count': len(self.active_agents),
            'total_agents': self.num_agents,
            'current_step': self.curr_step,
            'last_change_step': self.topology_config['last_change_step'],
            'topology_in_progress': self.topology_change['in_progress'],
            'change_type': self.topology_change['change_type']
        }

    def set_experiment_type(self, experiment_type):
        """åŠ¨æ€è®¾ç½®å®éªŒç±»å‹"""
        old_type = self.experiment_type
        self.experiment_type = experiment_type
        self._validate_experiment_type()
        self.topology_config = self._init_topology_config()
        print(f"å®éªŒç±»å‹å·²ä» '{old_type}' æ›´æ”¹ä¸º '{experiment_type}'")

    def _load_config(self, config, config_file, *args):
        """åŠ è½½é…ç½®"""
        try:
            from .config import ExperimentConfig, create_config, config_manager
        except ImportError:
            from config import ExperimentConfig, create_config, config_manager

        # å¦‚æœæä¾›äº†é…ç½®å¯¹è±¡
        if isinstance(config, ExperimentConfig):
            return config

        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶è·¯å¾„
        if config_file is not None:
            return config_manager.load_config(config_file)

        # å¦‚æœæä¾›äº†é…ç½®åç§°
        if isinstance(config, str):
            return create_config(config)

        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨ä¼ å…¥çš„å‚æ•°åˆ›å»ºé…ç½®
        if config is None:
            (num_agents, num_targets, world_size, coverage_radius,
             communication_radius, max_steps, render_mode, screen_size,
             render_fps, dt, experiment_type, topology_change_interval,
             topology_change_probability, min_active_agents, max_active_agents,
             initial_active_ratio) = args

            try:
                from .config import EnvironmentConfig, TopologyConfig, RewardConfig, PhysicsConfig, GATConfig
            except ImportError:
                from config import EnvironmentConfig, TopologyConfig, RewardConfig, PhysicsConfig, GATConfig

            # åˆ›å»ºé»˜è®¤é…ç½®å¹¶åº”ç”¨ä¼ å…¥çš„å‚æ•°
            env_config = EnvironmentConfig()
            if num_agents is not None: env_config.num_agents = num_agents
            if num_targets is not None: env_config.num_targets = num_targets
            if world_size is not None: env_config.world_size = world_size
            if coverage_radius is not None: env_config.coverage_radius = coverage_radius
            if communication_radius is not None: env_config.communication_radius = communication_radius
            if max_steps is not None: env_config.max_steps = max_steps
            if render_mode is not None: env_config.render_mode = render_mode
            if screen_size is not None: env_config.screen_size = screen_size
            if render_fps is not None: env_config.render_fps = render_fps
            if dt is not None: env_config.dt = dt

            topology_config = TopologyConfig()
            if experiment_type is not None: topology_config.experiment_type = experiment_type
            if topology_change_interval is not None: topology_config.topology_change_interval = topology_change_interval
            if topology_change_probability is not None: topology_config.topology_change_probability = topology_change_probability
            if min_active_agents is not None: topology_config.min_active_agents = min_active_agents
            if max_active_agents is not None: topology_config.max_active_agents = max_active_agents
            if initial_active_ratio is not None: topology_config.initial_active_ratio = initial_active_ratio

            return ExperimentConfig(
                name="runtime_config",
                description="è¿è¡Œæ—¶åˆ›å»ºçš„é…ç½®",
                environment=env_config,
                topology=topology_config,
                reward=RewardConfig(),
                physics=PhysicsConfig(),
                gat=GATConfig()
            )

        raise ValueError("å¿…é¡»æä¾›configã€config_fileæˆ–å…·ä½“å‚æ•°")

    def save_current_config(self, filename: str):
        """ä¿å­˜å½“å‰é…ç½®åˆ°æ–‡ä»¶"""
        config_manager.save_config(self.config, filename)

    def load_config_from_file(self, filename: str):
        """ä»æ–‡ä»¶é‡æ–°åŠ è½½é…ç½®å¹¶é‡æ–°åˆå§‹åŒ–ç¯å¢ƒ"""
        new_config = config_manager.load_config(filename)
        self.__init__(config=new_config)

    # ========== GATè®­ç»ƒç›¸å…³æ–¹æ³• ==========

    def get_gat_parameters(self):
        """è·å–GATç½‘ç»œçš„å‚æ•°ï¼Œç”¨äºå¤–éƒ¨è®­ç»ƒ"""
        return self.gat_network.parameters()

    def get_gat_named_parameters(self):
        """è·å–GATç½‘ç»œçš„å‘½åå‚æ•°"""
        return self.gat_network.named_parameters()

    def get_gat_state_dict(self):
        """è·å–GATç½‘ç»œçš„çŠ¶æ€å­—å…¸"""
        return self.gat_network.state_dict()

    def load_gat_state_dict(self, state_dict):
        """åŠ è½½GATç½‘ç»œçš„çŠ¶æ€å­—å…¸"""
        self.gat_network.load_state_dict(state_dict)

    def save_gat_model(self, filepath):
        """ä¿å­˜GATæ¨¡å‹"""
        torch.save(self.gat_network.state_dict(), filepath)
        print(f"GATæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

    def load_gat_model(self, filepath):
        """åŠ è½½GATæ¨¡å‹"""
        self.gat_network.load_state_dict(torch.load(filepath, map_location=self.device))
        print(f"GATæ¨¡å‹å·²ä» {filepath} åŠ è½½")

    def get_gat_features_with_grad(self, return_loss=False):
        """
        è·å–å¸¦æ¢¯åº¦çš„GATç‰¹å¾ï¼Œç”¨äºç«¯åˆ°ç«¯è®­ç»ƒ

        Args:
            return_loss: æ˜¯å¦è¿”å›GATçš„å†…éƒ¨æŸå¤±

        Returns:
            gat_features: å¸¦æ¢¯åº¦çš„GATç‰¹å¾
            loss: GATå†…éƒ¨æŸå¤±ï¼ˆå¦‚æœreturn_loss=Trueï¼‰
        """
        # åŸºç¡€ç‰¹å¾ï¼šä½ç½®å’Œé€Ÿåº¦
        active_positions = self.agent_pos[self.active_agents]
        active_velocities = self.agent_vel[self.active_agents]

        uav_features = torch.tensor(
            np.concatenate([active_positions, active_velocities], axis=1),
            dtype=torch.float32,
            device=self.device,
            requires_grad=True  # å¯ç”¨æ¢¯åº¦
        )
        target_features = torch.tensor(
            self.target_pos,
            dtype=torch.float32,
            device=self.device,
            requires_grad=True  # å¯ç”¨æ¢¯åº¦
        )

        # è·å–æ´»è·ƒUAVçš„é‚»æ¥çŸ©é˜µ
        active_uav_adj = self._get_active_adj_matrix()
        active_target_adj = self._get_active_target_adj_matrix()

        # GATå‰å‘ä¼ æ’­ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
        gat_features = self.gat_network(
            uav_features,
            target_features,
            active_uav_adj,
            active_target_adj
        )

        if return_loss:
            # è®¡ç®—GATçš„å†…éƒ¨æŸå¤±ï¼ˆå¯é€‰ï¼‰
            # è¿™é‡Œå¯ä»¥æ·»åŠ GATç‰¹å®šçš„æŸå¤±ï¼Œæ¯”å¦‚æ³¨æ„åŠ›æ­£åˆ™åŒ–
            attention_loss = 0.0  # å ä½ç¬¦
            return gat_features, attention_loss

        return gat_features

    def update_gat_with_loss(self, loss, optimizer):
        """
        ä½¿ç”¨ç»™å®šçš„æŸå¤±æ›´æ–°GATå‚æ•°

        Args:
            loss: åŒ…å«GATæ¢¯åº¦çš„æŸå¤±
            optimizer: GATçš„ä¼˜åŒ–å™¨
        """
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    def get_gat_info(self):
        """è·å–GATç½‘ç»œä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.gat_network.parameters())
        trainable_params = sum(p.numel() for p in self.gat_network.parameters() if p.requires_grad)

        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'device': str(self.device),
            'training_mode': self.gat_network.training,
            'model_structure': str(self.gat_network)
        }
#