"""
ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£å’ŒåŠŸèƒ½
åŸºäºåŸç‰ˆuav_env_top.pyï¼Œåˆ é™¤å†—ä½™ä»£ç ï¼Œä¿ç•™æ ¸å¿ƒåŠŸèƒ½
"""

import numpy as np
import pygame
import torch
import gym
from gym import spaces
import cv2

# åŠ¨æ€å¯¼å…¥GATæ¨¡å‹
import importlib.util
import os

def _import_gat_model():
    """åŠ¨æ€å¯¼å…¥GATæ¨¡å‹"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    gat_model_path = os.path.join(current_dir, 'gat_model_top.py')

    spec = importlib.util.spec_from_file_location("gat_model_top", gat_model_path)
    gat_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(gat_module)

    return gat_module.UAVAttentionNetwork, gat_module.create_adjacency_matrices

UAVAttentionNetwork, create_adjacency_matrices = _import_gat_model()

# ç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦å¤æ‚çš„é…ç½®å¯¼å…¥


class UAVEnv(gym.Env):
    """ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
    
    def __init__(self,
                 render_mode=None,
                 experiment_type='normal',
                 num_agents=6,
                 num_targets=10,
                 max_steps=200,
                 min_active_agents=3,
                 max_active_agents=None):
        
        # åŸºç¡€å‚æ•°è®¾ç½®
        self.num_agents = num_agents
        self.num_targets = num_targets
        self.world_size = 1.0
        self.max_steps = max_steps
        self.experiment_type = experiment_type
        self.render_mode = render_mode
        self.curr_step = 0
        self.max_coverage_rate = 0.0
        
        # ç‰©ç†å‚æ•°ï¼ˆä¸åŸç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
        self.max_speed = 2.0                    
        self.max_accel = 1.5   #å®é™…ä¸Šæ²¡æœ‰ç”¨ä¸Šè¿™ä¸ªåŠ é€Ÿåº¦çš„åŠŸèƒ½                 
        self.communication_range = 0.8          # åŸç‰ˆæœ¬: 0.6
        self.coverage_radius = 0.4              # åŸç‰ˆæœ¬: 0.3 (sensing_rangeæ”¹å)
        self.sensing_range = 0.4              # ä¸coverage_radiusä¿æŒä¸€è‡´
        self.dt = 0.1                           # åŸç‰ˆæœ¬: 0.1
        
        # æ‹“æ‰‘å‚æ•°
        self.min_active_agents = min_active_agents
        self.max_active_agents = max_active_agents or num_agents
        
        # å¥–åŠ±æƒé‡ï¼ˆä¿æŒåŸæœ‰è®¾ç½®ï¼‰
        self.coverage_weight = 3.5
        self.connectivity_weight = 2.0
        self.boundary_weight = 1.0
        self.stability_weight = 1.5
        
        # æ¦‚ç‡è®¾ç½®ï¼ˆç”¨äºprobabilisticæ¨¡å¼ï¼‰
        self.normal_probability = 0.60
        self.loss_probability = 0.25
        self.addition_probability = 0.15
        
        # çŠ¶æ€å˜é‡
        self.curr_step = 0
        self.active_agents = list(range(self.num_agents))
        self.agents = [f"agent_{i}" for i in range(self.num_agents)]
        
        # ä½ç½®å’Œé€Ÿåº¦
        self.agent_pos = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.target_pos = np.zeros((self.num_targets, 2), dtype=np.float32)
        
        # Episodeè®¡åˆ’
        self.episode_plan = {
            'type': 'normal',
            'trigger_step': None,
            'executed': False
        }

        # GATç¼“å­˜ä¼˜åŒ– - å‡å°‘CPU-GPUä¼ è¾“
        self.gat_cache = {
            'features': None,
            'last_update_step': -1,
            'update_interval': 3  # æ¯3æ­¥æ›´æ–°ä¸€æ¬¡GATç‰¹å¾
        }

        # é€Ÿåº¦é™åˆ¶å‚æ•° - åŸºäºè¿æ¥æ€§çš„åŠ¨æ€çº¦æŸ
        self.max_base_speed = 1.0           # åŸºç¡€æœ€å¤§é€Ÿåº¦ï¼ˆä¸max_speedä¸€è‡´ï¼‰
        self.connectivity_speed_factor = 0.5 # è¿æ¥æ€§å½±å“å› å­
        self.min_speed_limit = 0.2          # æœ€å°é€Ÿåº¦é™åˆ¶ï¼ˆè°ƒæ•´æ¯”ä¾‹ï¼‰
        self.epsilon = 0.1                  # è¿æ¥æ€§å®¹å¿åº¦å‚æ•°ï¼ˆè°ƒæ•´æ¯”ä¾‹ï¼‰
        self.speed_violation_penalty = -5.0 # é€Ÿåº¦è¿è§„æƒ©ç½š

        # è¿æ¥æ€§å†å²è®°å½•
        self.connectivity_history = []
        self.speed_limits_history = []
        
        # GATç½‘ç»œ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gat_model = UAVAttentionNetwork(
            uav_features=4,      # UAVç‰¹å¾ç»´åº¦ï¼šä½ç½®(2) + é€Ÿåº¦(2)
            target_features=2,   # ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼šä½ç½®(2)
            hidden_size=64,      # éšè—å±‚å¤§å°
            heads=4,             # æ³¨æ„åŠ›å¤´æ•°
            dropout=0.1,         # dropoutç‡
            device=self.device
        )
        
        # è®­ç»ƒæ¨¡å¼æ ‡å¿—
        self.training = False
        
        # è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
        self._setup_spaces()
        
        # æ¸²æŸ“ç›¸å…³
        self.screen = None
        self.clock = None
        self.width, self.height = (700, 700)  # ä¿æŒä¸åŸç‰ˆæœ¬ä¸€è‡´
        self.metadata = {"render_fps": 60}  # ä¸åŸç‰ˆæœ¬ä¸€è‡´
        self.font = None  # å­—ä½“å¯¹è±¡
        
        # å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åŠ¨ä½œå¹³æ»‘å‚æ•°
        self.action_smooth_alpha = 0.7  # å¹³æ»‘ç³»æ•°ï¼Œ0.7è¡¨ç¤º70%æ–°åŠ¨ä½œ+30%æ—§åŠ¨ä½œ
        self.enable_action_smoothing = True  # æ˜¯å¦å¯ç”¨å¹³æ»‘
        
    def _setup_spaces(self):
        """è®¾ç½®è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ - ä¿æŒåŸæœ‰æ ¼å¼"""
        # è§‚å¯Ÿç»´åº¦è®¡ç®—ï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
        obs_dim = (
            4 +                           # åŸºç¡€çŠ¶æ€ (ä½ç½®+é€Ÿåº¦)
            2 * self.num_targets +       # ç›®æ ‡ç›¸å¯¹ä½ç½®
            2 * (self.num_agents - 1) +  # é‚»å±…ä¿¡æ¯
            32 +                         # GATç‰¹å¾
            3                            # æ‹“æ‰‘ä¿¡æ¯
        )
        
        # ä¿æŒåŸæœ‰çš„å­—å…¸æ ¼å¼
        self.observation_spaces = {
            agent: spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
            for agent in self.agents
        }
        
        self.action_spaces = {
            agent: spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)
            for agent in self.agents
        }
    
    def reset(self, seed=None):
        """é‡ç½®ç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
        if seed is not None:
            np.random.seed(seed)
        
        self.curr_step = 0
        self.max_coverage_rate = 0.0
        self.active_agents = list(range(self.num_agents))
        
        # åˆå§‹åŒ–UAVä½ç½®ï¼ˆä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼šåº•éƒ¨æ’åˆ—ï¼‰
        self.agent_pos = []
        bottom_y = -self.world_size + 0.15
        spacing = 2 * self.world_size / (self.num_agents + 1)
        for i in range(self.num_agents):
            x = -self.world_size + (i + 1) * spacing
            self.agent_pos.append([x, bottom_y])
        self.agent_pos = np.array(self.agent_pos, dtype=np.float32)

        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åˆå§‹åŒ–ç›®æ ‡ä½ç½®ï¼ˆä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼š0.85èŒƒå›´å†…éšæœºåˆ†å¸ƒï¼‰
        self.target_pos = np.random.uniform(-self.world_size*0.85, self.world_size*0.85,
                                          (self.num_targets, 2)).astype(np.float32)
        
        # é‡ç½®å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # é‡ç½®GATç¼“å­˜
        self.gat_cache['features'] = None
        self.gat_cache['last_update_step'] = -1

        # é‡ç½®é€Ÿåº¦é™åˆ¶å†å²
        self.connectivity_history = []
        self.speed_limits_history = []

        # åˆ¶å®šepisodeè®¡åˆ’
        self._plan_episode()
        
        # è¿”å›åŸæœ‰æ ¼å¼çš„è§‚å¯Ÿ
        obs_list = self._get_obs()
        return {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}, {}
    
    def _plan_episode(self):
        """åˆ¶å®šepisodeæ‹“æ‰‘å˜åŒ–è®¡åˆ’"""
        if self.experiment_type != 'probabilistic':
            self.episode_plan = {'type': 'normal', 'trigger_step': None, 'executed': False}
            return
        
        # æ ¹æ®æ¦‚ç‡å†³å®šepisodeç±»å‹
        rand = np.random.random()
        
        if rand < self.normal_probability:
            episode_type = 'normal'
            trigger_step = None
        elif rand < self.normal_probability + self.loss_probability:
            episode_type = 'loss'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        else:
            episode_type = 'addition'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        
        self.episode_plan = {
            'type': episode_type,
            'trigger_step': trigger_step,
            'executed': False
        }
        
        print(f"ğŸ“‹ Episodeè®¡åˆ’: {episode_type}" + 
              (f" (ç¬¬{trigger_step}æ­¥è§¦å‘)" if trigger_step else ""))
    
    def step(self, actions):
        """æ‰§è¡Œä¸€æ­¥ - æ·»åŠ åŸºäºè¿æ¥æ€§çš„é€Ÿåº¦é™åˆ¶"""
        self.curr_step += 1

        # åŠ¨ä½œå¹³æ»‘å¤„ç†
        if self.enable_action_smoothing:
            actions = self._smooth_actions(actions)

        # è®¡ç®—å½“å‰æ­¥çš„åŠ¨æ€é€Ÿåº¦é™åˆ¶
        speed_limits = self._compute_connectivity_based_speed_limits()
        self.speed_limits_history.append(speed_limits.copy())

        # è®°å½•é€Ÿåº¦è¿è§„æƒ…å†µ
        speed_violations = {}

        # æ‰§è¡ŒåŠ¨ä½œï¼ˆåº”ç”¨é€Ÿåº¦é™åˆ¶ï¼‰
        for i, agent in enumerate(self.agents):
            if i in self.active_agents and agent in actions:
                action = np.array(actions[agent], dtype=np.float32)
                action = np.clip(action, -1.0, 1.0)

                # è®¡ç®—æœŸæœ›é€Ÿåº¦
                desired_velocity = action * self.max_speed
                desired_speed = np.linalg.norm(desired_velocity)

                # åº”ç”¨åŠ¨æ€é€Ÿåº¦é™åˆ¶
                speed_limit = speed_limits[i]
                if desired_speed > speed_limit:
                    # é€Ÿåº¦è¶…é™ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
                    if desired_speed > 0:
                        scale_factor = speed_limit / desired_speed
                        actual_velocity = desired_velocity * scale_factor
                    else:
                        actual_velocity = desired_velocity

                    # è®°å½•è¿è§„
                    speed_violations[agent] = {
                        'desired_speed': desired_speed,
                        'limit': speed_limit,
                        'violation': desired_speed - speed_limit
                    }
                else:
                    actual_velocity = desired_velocity
                    speed_violations[agent] = {
                        'desired_speed': desired_speed,
                        'limit': speed_limit,
                        'violation': 0.0
                    }

                # æ›´æ–°é€Ÿåº¦å’Œä½ç½®ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç‰©ç†æ¨¡å‹ï¼‰
                self.agent_vel[i] = actual_velocity
                self.agent_pos[i] += self.agent_vel[i] * self.dt  # ä½ç½® = ä½ç½® + é€Ÿåº¦ Ã— æ—¶é—´

                # è¾¹ç•Œå¤„ç†
                self.agent_pos[i] = np.clip(self.agent_pos[i],
                                          -self.world_size, self.world_size)

                # è®°å½•åŠ¨ä½œï¼ˆè®°å½•å¹³æ»‘åçš„åŠ¨ä½œç”¨äºä¸‹ä¸€æ­¥ï¼‰
                self.prev_actions[i] = action
        
        # æ£€æŸ¥æ‹“æ‰‘å˜åŒ–
        self._check_topology_change()

        # è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«é€Ÿåº¦é™åˆ¶å¥–åŠ±ï¼‰
        rewards = self._compute_rewards(speed_violations)
        
        # æ£€æŸ¥ç»“æŸæ¡ä»¶
        dones = {agent: self.curr_step >= self.max_steps for agent in self.agents}
        truncated = dones.copy()
        
        # è·å–è§‚å¯Ÿ
        obs_list = self._get_obs()
        observations = {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}
        
        return observations, rewards, dones, truncated, {}
    
    def _check_topology_change(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œæ‹“æ‰‘å˜åŒ–"""
        if (self.episode_plan['type'] != 'normal' and 
            not self.episode_plan['executed'] and 
            self.curr_step >= self.episode_plan['trigger_step']):
            
            if self.episode_plan['type'] == 'loss':
                success = self._execute_uav_loss()
            elif self.episode_plan['type'] == 'addition':
                success = self._execute_uav_addition()
            else:
                success = False
            
            if success:
                self.episode_plan['executed'] = True
                print(f"ğŸ¯ æ‰§è¡Œè®¡åˆ’: {self.episode_plan['type']} (ç¬¬{self.curr_step}æ­¥)")
    
    def _execute_uav_loss(self):
        """æ‰§è¡ŒUAVæŸå¤±"""
        if len(self.active_agents) <= self.min_active_agents:
            return False
        
        lost_uav = np.random.choice(self.active_agents)
        self.active_agents.remove(lost_uav)
        
        print(f"UAV {lost_uav} å·²å¤±æ•ˆï¼Œå½“å‰ä½ç½®: {self.agent_pos[lost_uav]}")
        print(f"[å®éªŒæ¨¡å¼: UAVæŸå¤±] Step {self.curr_step}: UAV {lost_uav} å¤±æ•ˆ")
        return True
    
    def _execute_uav_addition(self):
        """æ‰§è¡ŒUAVæ·»åŠ """
        if len(self.active_agents) >= self.max_active_agents:
            return False
        
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if not inactive_uavs:
            return False
        
        new_uav = np.random.choice(inactive_uavs)
        self.active_agents.append(new_uav)
        
        # é‡æ–°åˆå§‹åŒ–ä½ç½®
        self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
        self.agent_vel[new_uav] = np.zeros(2)
        
        print(f"[å®éªŒæ¨¡å¼: UAVæ·»åŠ ] Step {self.curr_step}: æ·»åŠ æ–°UAV {new_uav}")
        return True

    def _get_obs(self):
        """è·å–è§‚å¯Ÿ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘é‡å¤è®¡ç®—"""
        # è®¡ç®—GATç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        gat_features = self._compute_gat_features()

        # é¢„è®¡ç®—å…¬å…±æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
        active_agents_ratio = len(self.active_agents) / self.num_agents
        time_progress = self.curr_step / self.max_steps

        # æ‰¹é‡è®¡ç®—ç›®æ ‡ç›¸å¯¹ä½ç½®
        target_relative_all = []
        for i in range(self.num_agents):
            target_relative = (self.target_pos - self.agent_pos[i]).flatten()
            target_relative_all.append(target_relative)

        # æ‰¹é‡è®¡ç®—é‚»å±…ç›¸å¯¹ä½ç½®
        neighbor_relative_all = []
        for i in range(self.num_agents):
            neighbor_relative = []
            for j in range(self.num_agents):
                if j != i:
                    if j in self.active_agents:
                        relative_pos = self.agent_pos[j] - self.agent_pos[i]
                        neighbor_relative.extend(relative_pos)
                    else:
                        neighbor_relative.extend([0.0, 0.0])
            neighbor_relative_all.append(neighbor_relative)

        # æ‰¹é‡è½¬æ¢GATç‰¹å¾
        gat_features_np = []
        for i in range(self.num_agents):
            if i < len(gat_features):
                gat_feat = gat_features[i]
                if torch.is_tensor(gat_feat):
                    if gat_feat.requires_grad:
                        gat_feat = gat_feat.detach().cpu().numpy()
                    else:
                        gat_feat = gat_feat.cpu().numpy()
                gat_features_np.append(gat_feat)
            else:
                gat_features_np.append(np.zeros(32, dtype=np.float32))

        # ç»„è£…è§‚å¯Ÿ
        obs_list = []
        for i in range(self.num_agents):
            obs_parts = np.concatenate([
                self.agent_pos[i],                    # ä½ç½®
                self.agent_vel[i],                    # é€Ÿåº¦
                target_relative_all[i],               # ç›®æ ‡ç›¸å¯¹ä½ç½®
                neighbor_relative_all[i],             # é‚»å±…ä¿¡æ¯
                gat_features_np[i],                   # GATç‰¹å¾
                [active_agents_ratio,                 # æ´»è·ƒæ¯”ä¾‹
                 float(i in self.active_agents),      # è‡ªèº«çŠ¶æ€
                 time_progress]                       # æ—¶é—´è¿›åº¦
            ]).astype(np.float32)

            obs_list.append(obs_parts)

        return obs_list

    def _compute_gat_features(self):
        """è®¡ç®—GATç‰¹å¾ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘CPU-GPUä¼ è¾“"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°GATç‰¹å¾ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰
        if (self.gat_cache['features'] is not None and
            self.curr_step - self.gat_cache['last_update_step'] < self.gat_cache['update_interval']):
            return self.gat_cache['features']

        # å‡†å¤‡è¾“å…¥æ•°æ® - æ‰¹é‡è½¬æ¢å‡å°‘ä¼ è¾“æ¬¡æ•°
        uav_features = np.concatenate([self.agent_pos, self.agent_vel], axis=1)

        # ä¸€æ¬¡æ€§ä¼ è¾“æ‰€æœ‰æ•°æ®åˆ°GPU
        with torch.no_grad():
            uav_tensor = torch.FloatTensor(uav_features).to(self.device, non_blocking=True)
            target_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)
            uav_pos_tensor = torch.FloatTensor(self.agent_pos).to(self.device, non_blocking=True)
            target_pos_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)

        # åˆ›å»ºé‚»æ¥çŸ©é˜µ
        uav_adj, uav_target_adj = create_adjacency_matrices(
            uav_pos_tensor, target_pos_tensor,
            self.communication_range, self.sensing_range,
            active_uavs=self.active_agents
        )

        # GATå‰å‘ä¼ æ’­
        with torch.set_grad_enabled(self.training):
            gat_features = self.gat_model(uav_tensor, target_tensor,
                                        uav_adj, uav_target_adj,
                                        active_agents=self.active_agents)

        if not self.training:
            gat_features = gat_features.detach()

        # æ›´æ–°ç¼“å­˜
        self.gat_cache['features'] = gat_features
        self.gat_cache['last_update_step'] = self.curr_step

        return gat_features

    def _compute_connectivity_based_speed_limits(self):
        """
        åŸºäºè¿æ¥æ€§è®¡ç®—æ¯ä¸ªUAVçš„åŠ¨æ€é€Ÿåº¦é™åˆ¶
        å®ç°è®ºæ–‡å¼•ç†1çš„è¿æ¥æ€§ä¿æŒçº¦æŸ
        """
        speed_limits = np.full(self.num_agents, self.max_base_speed, dtype=np.float32)

        # è®¡ç®—å½“å‰è¿æ¥æ€§çŸ©é˜µ
        connectivity_matrix = self._compute_connectivity_matrix()

        for i in range(self.num_agents):
            if i not in self.active_agents:
                speed_limits[i] = 0.0
                continue

            # æ‰¾åˆ°å…³é”®é‚»å±…é›†åˆ N_i^c (critical neighbors)
            critical_neighbors = self._find_critical_neighbors(i, connectivity_matrix)

            if len(critical_neighbors) == 0:
                # æ²¡æœ‰å…³é”®é‚»å±…ï¼Œä½¿ç”¨åŸºç¡€é€Ÿåº¦é™åˆ¶
                speed_limits[i] = self.max_base_speed
            else:
                # è®¡ç®—åŸºäºé‚»å±…è·ç¦»çš„é€Ÿåº¦çº¦æŸ
                min_constraint = float('inf')

                for j in critical_neighbors:
                    if j in self.active_agents:
                        # è®¡ç®—åˆ°é‚»å±…jçš„è·ç¦»
                        dist_ij = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])

                        # åŸºäºå¼•ç†1çš„çº¦æŸè®¡ç®—
                        # Îµ_i = min(r_c - d_ij) â‰¤ Îµ
                        epsilon_i = min(self.communication_range - dist_ij, self.epsilon)

                        if epsilon_i > 0:
                            # Î”d_i â‰¤ Îµ_i/2 (å½“N_i^c â‰  âˆ…æ—¶)
                            constraint = epsilon_i / 2.0
                            min_constraint = min(min_constraint, constraint)

                if min_constraint != float('inf'):
                    speed_limits[i] = max(min_constraint, self.min_speed_limit)
                else:
                    speed_limits[i] = self.max_base_speed

        return speed_limits

    def _compute_connectivity_matrix(self):
        """è®¡ç®—è¿æ¥æ€§çŸ©é˜µ"""
        n = self.num_agents
        connectivity_matrix = np.zeros((n, n), dtype=bool)

        for i in range(n):
            for j in range(i+1, n):
                if i in self.active_agents and j in self.active_agents:
                    dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                    if dist <= self.communication_range:
                        connectivity_matrix[i, j] = True
                        connectivity_matrix[j, i] = True

        return connectivity_matrix

    def _find_critical_neighbors(self, agent_id, connectivity_matrix):
        """
        æ‰¾åˆ°agentçš„å…³é”®é‚»å±…é›†åˆ
        å…³é”®é‚»å±…æ˜¯é‚£äº›å¯¹ä¿æŒå…¨å±€è¿é€šæ€§è‡³å…³é‡è¦çš„é‚»å±…
        """
        critical_neighbors = []

        # è·å–å½“å‰é‚»å±…
        current_neighbors = np.where(connectivity_matrix[agent_id])[0]

        for neighbor in current_neighbors:
            if neighbor in self.active_agents:
                # æ£€æŸ¥ç§»é™¤è¿™ä¸ªè¿æ¥æ˜¯å¦ä¼šå½±å“å…¨å±€è¿é€šæ€§
                temp_matrix = connectivity_matrix.copy()
                temp_matrix[agent_id, neighbor] = False
                temp_matrix[neighbor, agent_id] = False

                if not self._is_graph_connected(temp_matrix):
                    critical_neighbors.append(neighbor)

        return critical_neighbors

    def _is_graph_connected(self, adjacency_matrix):
        """æ£€æŸ¥å›¾æ˜¯å¦è¿é€šï¼ˆä½¿ç”¨DFSï¼‰"""
        active_indices = [i for i in range(self.num_agents) if i in self.active_agents]

        if len(active_indices) <= 1:
            return True

        # ä»ç¬¬ä¸€ä¸ªæ´»è·ƒèŠ‚ç‚¹å¼€å§‹DFS
        visited = set()
        stack = [active_indices[0]]

        while stack:
            node = stack.pop()
            if node in visited:
                continue
            visited.add(node)

            # æ·»åŠ æ‰€æœ‰è¿æ¥çš„é‚»å±…
            for neighbor in range(self.num_agents):
                if (neighbor in self.active_agents and
                    neighbor not in visited and
                    adjacency_matrix[node, neighbor]):
                    stack.append(neighbor)

        return len(visited) == len(active_indices)

    def _compute_rewards(self, speed_violations=None):
        """è®¡ç®—å¥–åŠ± - åŒ…å«é€Ÿåº¦é™åˆ¶ç›¸å…³å¥–åŠ±"""
        rewards = {}

        # è®¡ç®—è¦†ç›–ç‡
        coverage_rate, _, _, _ = self.calculate_coverage_complete()

        # è®¡ç®—è¿é€šæ€§
        connectivity_reward = self._compute_connectivity_reward()

        # è®¡ç®—ç¨³å®šæ€§
        stability_reward = self._compute_stability_reward()

        # è®¡ç®—é€Ÿåº¦åˆè§„å¥–åŠ±
        speed_compliance_reward = self._compute_speed_compliance_reward(speed_violations)

        # åŸºç¡€å¥–åŠ±ï¼ˆåŠ å…¥é€Ÿåº¦åˆè§„ï¼‰
        base_reward = (
            self.coverage_weight * coverage_rate +
            self.connectivity_weight * connectivity_reward +
            self.stability_weight * stability_reward +
            0.1 * speed_compliance_reward  # é€Ÿåº¦åˆè§„æƒé‡
        )

        for i, agent in enumerate(self.agents):
            if i in self.active_agents:
                reward = base_reward

                # è¾¹ç•Œæƒ©ç½š
                if np.any(np.abs(self.agent_pos[i]) > self.world_size - 0.1):
                    reward -= self.boundary_weight

                # ä¸ªä½“é€Ÿåº¦è¿è§„æƒ©ç½š
                if speed_violations and agent in speed_violations:
                    violation = speed_violations[agent]['violation']
                    if violation > 0:
                        reward += self.speed_violation_penalty * violation

                rewards[agent] = reward
            else:
                rewards[agent] = 0.0

        return rewards

    def _compute_speed_compliance_reward(self, speed_violations):
        """è®¡ç®—é€Ÿåº¦åˆè§„å¥–åŠ±"""
        if not speed_violations:
            return 0.0

        total_compliance = 0.0
        active_count = 0

        for agent in self.agents:
            if agent in speed_violations:
                violation = speed_violations[agent]['violation']
                # åˆè§„åº¦ = 1 - (è¿è§„ç¨‹åº¦ / æœ€å¤§å¯èƒ½è¿è§„)
                compliance = max(0.0, 1.0 - violation / self.max_base_speed)
                total_compliance += compliance
                active_count += 1

        return total_compliance / max(active_count, 1)

    def _smooth_actions(self, actions):
        """åŠ¨ä½œå¹³æ»‘å¤„ç† - ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
        smoothed_actions = {}

        for i, agent in enumerate(self.agents):
            if agent in actions and i < len(self.prev_actions):
                raw_action = np.array(actions[agent], dtype=np.float32)
                prev_action = self.prev_actions[i]

                # æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘
                smooth_action = (self.action_smooth_alpha * raw_action +
                               (1 - self.action_smooth_alpha) * prev_action)

                smoothed_actions[agent] = smooth_action
            else:
                # å¦‚æœæ²¡æœ‰å†å²åŠ¨ä½œæˆ–æ™ºèƒ½ä½“ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸåŠ¨ä½œ
                smoothed_actions[agent] = actions[agent] if agent in actions else np.zeros(2)

        return smoothed_actions

    def close(self):
        """å…³é—­ç¯å¢ƒ - ä¸åŸç‰ˆæœ¬ä¸€è‡´"""
        if self.screen is not None:
            pygame.quit()
            self.screen = None

    def calculate_coverage_complete(self):
        """è®¡ç®—å®Œæ•´è¦†ç›–ç‡ä¿¡æ¯ - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        # è®¡ç®—ç›®æ ‡ç‚¹æ˜¯å¦è¢«è¦†ç›–
        covered_flags = []
        for target in self.target_pos:
            covered = False
            for agent in self.agent_pos:
                distance = np.linalg.norm(target - agent)
                if distance <= self.coverage_radius:
                    covered = True
                    break
            covered_flags.append(covered)

        # è®¡ç®—è¦†ç›–ç‡
        covered_count = sum(covered_flags)
        total_targets = len(self.target_pos)
        coverage_rate = covered_count / total_targets if total_targets > 0 else 0

        # æ„å»ºé€šä¿¡é‚»æ¥çŸ©é˜µ
        num_agents = self.num_agents
        adjacency_matrix = np.zeros((num_agents, num_agents), dtype=bool)
        for i in range(num_agents):
            for j in range(i + 1, num_agents):
                distance = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                if distance <= self.communication_range:
                    adjacency_matrix[i, j] = True
                    adjacency_matrix[j, i] = True

        # DFS æ£€æŸ¥è¿é€šæ€§
        visited = [False] * num_agents

        def dfs(idx):
            visited[idx] = True
            for neighbor_idx, connected in enumerate(adjacency_matrix[idx]):
                if connected and not visited[neighbor_idx]:
                    dfs(neighbor_idx)

        dfs(0)
        fully_connected = all(visited)
        unconnected_count = visited.count(False)

        # æ›´æ–°æœ€å¤§è¦†ç›–ç‡
        if fully_connected:
            self.max_coverage_rate = max(self.max_coverage_rate, coverage_rate)

        return coverage_rate, fully_connected, self.max_coverage_rate, unconnected_count

    def _compute_connectivity_reward(self):
        """è®¡ç®—è¿é€šæ€§å¥–åŠ±"""
        if len(self.active_agents) <= 1:
            return 1.0

        connected_count = 0
        for i in self.active_agents:
            for j in self.active_agents:
                if i != j:
                    distance = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                    if distance <= self.communication_range:
                        connected_count += 1
                        break

        return connected_count / len(self.active_agents)

    def _compute_stability_reward(self):
        """è®¡ç®—ç¨³å®šæ€§å¥–åŠ±"""
        # ç®€åŒ–çš„ç¨³å®šæ€§è®¡ç®—
        if len(self.coverage_history) < 2:
            return 0.0

        recent_coverage = self.coverage_history[-10:] if len(self.coverage_history) >= 10 else self.coverage_history
        if len(recent_coverage) < 2:
            return 0.0

        stability = 1.0 - np.std(recent_coverage)
        return max(0.0, stability)

    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        if self.render_mode is None:
            import gym
            gym.logger.warn(
                "Calling render without specifying render_mode."
            )
            return
        if self.screen is None:
            pygame.init()
            pygame.font.init()  # ç¡®ä¿å­—ä½“æ¨¡å—åˆå§‹åŒ–
            self.screen = pygame.display.set_mode((self.width, self.height))
            pygame.display.set_caption('UAV Topology')
            self.clock = pygame.time.Clock()

            # åˆå§‹åŒ–å­—ä½“
            try:
                # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                self.font = pygame.font.SysFont('arial', 24, bold=True)
                if self.font is None:
                    raise Exception("SysFont failed")
            except:
                try:
                    # å¤‡é€‰ï¼šä½¿ç”¨é»˜è®¤å­—ä½“
                    self.font = pygame.font.Font(None, 28)
                except:
                    # æœ€åå¤‡é€‰ï¼šåˆ›å»ºåŸºç¡€å­—ä½“
                    self.font = pygame.font.Font(pygame.font.get_default_font(), 24)
        self.draw()
        if self.render_mode == 'rgb_array':
            data = pygame.surfarray.array3d(self.screen)
            return np.transpose(data, (1, 0, 2))
        elif self.render_mode == 'human':
            pygame.display.flip()
            self.clock.tick(self.metadata['render_fps'])

    def draw(self):
        """ç»˜åˆ¶å‡½æ•° - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        fixed_cam = self.world_size
        self.screen.fill((255, 255, 255))

        def to_screen(pos):
            x, y = pos
            y = -y  # y è½´ç¿»è½¬
            sx = int((x / fixed_cam) * (self.width / 2) + self.width / 2)
            sy = int((y / fixed_cam) * (self.height / 2) + self.height / 2)
            return sx, sy

        # ç”»ç›®æ ‡ç‚¹
        for tpos in self.target_pos:
            sx, sy = to_screen(tpos)
            pygame.draw.circle(self.screen, (0, 255, 0), (sx, sy), 5)

        # å­˜å‚¨æ— äººæœºå±å¹•ä½ç½®ç”¨äºè¿çº¿
        screen_positions = []

        for i, apos in enumerate(self.agent_pos):
            sx, sy = to_screen(apos)
            screen_positions.append((sx, sy))

            # ç»˜åˆ¶æ— äººæœºå›¾åƒæˆ–é»˜è®¤å›¾å½¢
            if not hasattr(self, 'drone_image'):
                path = os.path.dirname(__file__)
                img = os.path.join(path, 'UAV.png')
                if os.path.exists(img):
                    self.drone_image = pygame.transform.scale(pygame.image.load(img), (30, 30))
                else:
                    self.drone_image = None
            if self.drone_image:
                rect = self.drone_image.get_rect(center=(sx, sy))
                self.screen.blit(self.drone_image, rect)
            else:
                # æ ¹æ®UAVæ˜¯å¦å¤±æ•ˆé€‰æ‹©é¢œè‰²
                if i in self.active_agents:
                    color = (0, 0, 255)  # è“è‰² - æ´»è·ƒUAV
                else:
                    color = (128, 128, 128)  # ç°è‰² - å¤±æ•ˆUAV
                pygame.draw.circle(self.screen, color, (sx, sy), 8)

            # ç»˜åˆ¶æ¢æµ‹åŠå¾„åœ†åœˆï¼ˆå®çº¿ï¼‰
            coverage_radius_px = int((self.coverage_radius / fixed_cam) * (self.width/2))
            pygame.draw.circle(self.screen, (0, 0, 255), (sx, sy), coverage_radius_px, 1)

            # ç»˜åˆ¶é€šä¿¡åŠå¾„åœ†åœˆï¼ˆè“è‰²è™šçº¿ï¼‰
            comm_radius_px = int((self.communication_range / fixed_cam) * (self.width/2))
            # åˆ›å»ºè™šçº¿æ•ˆæœ
            num_segments = 80  # è™šçº¿æ®µæ•°
            for j in range(num_segments):
                if j % 2 == 0:  # åªç”»å¶æ•°æ®µï¼Œå½¢æˆè™šçº¿
                    start_angle = 2 * np.pi * j / num_segments
                    end_angle = 2 * np.pi * (j + 1) / num_segments
                    # è®¡ç®—å¼§æ®µçš„èµ·ç‚¹å’Œç»ˆç‚¹
                    start_pos = (
                        sx + int(comm_radius_px * np.cos(start_angle)),
                        sy + int(comm_radius_px * np.sin(start_angle))
                    )
                    end_pos = (
                        sx + int(comm_radius_px * np.cos(end_angle)),
                        sy + int(comm_radius_px * np.sin(end_angle))
                    )
                    # ç”»è™šçº¿æ®µ
                    pygame.draw.line(self.screen, (70, 130, 180), start_pos, end_pos, 1)  # ä½¿ç”¨æµ…è“è‰²

        # ç”»çº¢çº¿ï¼šè‹¥ä¸¤ä¸ªæ— äººæœºä¹‹é—´çš„è·ç¦»å°äºé€šä¿¡èŒƒå›´
        for i in range(self.num_agents):
            for j in range(i + 1, self.num_agents):
                dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                if dist <= self.communication_range:
                    pygame.draw.line(self.screen, (255, 0, 0),
                                    screen_positions[i], screen_positions[j], 1)

        # æ˜¾ç¤ºæ–‡å­—ä¿¡æ¯ï¼ˆä½¿ç”¨é¢„åˆå§‹åŒ–çš„å­—ä½“ï¼‰
        if self.font is None:
            # å¦‚æœå­—ä½“æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ç®€å•å¤‡é€‰
            font = pygame.font.Font(None, 24)
        else:
            font = self.font

        # æ˜¾ç¤ºå®éªŒç±»å‹ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ç¼–ç é—®é¢˜ï¼‰
        experiment_text = f"Experiment: {self.experiment_type}"
        text_surface = font.render(experiment_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 10))

        # æ˜¾ç¤ºæ­¥æ•°
        step_text = f"Step: {self.curr_step}/{self.max_steps}"
        text_surface = font.render(step_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 40))

        # æ˜¾ç¤ºæ´»è·ƒUAVæ•°é‡
        uav_text = f"Active UAVs: {len(self.active_agents)}/{self.num_agents}"
        text_surface = font.render(uav_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 70))

        # æ˜¾ç¤ºè¦†ç›–ç‡ä¿¡æ¯
        coverage_rate, _, _, _ = self.calculate_coverage_complete()
        coverage_text = f"Coverage: {coverage_rate:.3f}"
        text_surface = font.render(coverage_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 100))

        # æ˜¾ç¤ºepisodeè®¡åˆ’ä¿¡æ¯
        if hasattr(self, 'episode_plan'):
            plan_type = self.episode_plan.get('type', 'unknown')
            trigger_step = self.episode_plan.get('trigger_step', None)
            executed = self.episode_plan.get('executed', False)

            plan_text = f"Plan: {plan_type}"
            if trigger_step:
                plan_text += f" (step {trigger_step})"
            if executed:
                plan_text += " [done]"

            text_surface = font.render(plan_text, True, (0, 0, 255))
            self.screen.blit(text_surface, (10, 130))



    # GATç›¸å…³æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_gat_parameters(self):
        """è·å–GATå‚æ•°"""
        return self.gat_model.parameters()

    def save_gat_model(self, path):
        """ä¿å­˜GATæ¨¡å‹"""
        torch.save(self.gat_model.state_dict(), path)

    def load_gat_model(self, path):
        """åŠ è½½GATæ¨¡å‹"""
        self.gat_model.load_state_dict(torch.load(path, map_location=self.device))

    # å…¼å®¹æ€§æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_observation_space(self, agent):
        """è·å–è§‚å¯Ÿç©ºé—´"""
        return self.observation_spaces[agent]

    def get_action_space(self, agent):
        """è·å–åŠ¨ä½œç©ºé—´"""
        return self.action_spaces[agent]

    def fail_uav(self, uav_idx):
        """ä½¿UAVå¤±æ•ˆ - å…¼å®¹æ€§æ–¹æ³•"""
        if uav_idx in self.active_agents:
            self.active_agents.remove(uav_idx)
            # æ¸…é›¶å¤±æ•ˆUAVçš„å†å²åŠ¨ä½œï¼Œé¿å…å½±å“å¹³æ»‘
            if uav_idx < len(self.prev_actions):
                self.prev_actions[uav_idx] = np.zeros(2, dtype=np.float32)
            return True
        return False

    def add_uav(self):
        """æ·»åŠ UAV - å…¼å®¹æ€§æ–¹æ³•"""
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if inactive_uavs:
            new_uav = inactive_uavs[0]
            self.active_agents.append(new_uav)
            self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
            self.agent_vel[new_uav] = np.zeros(2)
            # é‡ç½®æ–°æ¿€æ´»UAVçš„å†å²åŠ¨ä½œ
            if new_uav < len(self.prev_actions):
                self.prev_actions[new_uav] = np.zeros(2, dtype=np.float32)
            return new_uav
        return None

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.screen is not None:
            pygame.display.quit()
            pygame.quit()
            self.screen = None
            self.clock = None
