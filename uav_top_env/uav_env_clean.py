"""
ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£å’ŒåŠŸèƒ½
åŸºäºåŸç‰ˆuav_env_top.pyï¼Œåˆ é™¤å†—ä½™ä»£ç ï¼Œä¿ç•™æ ¸å¿ƒåŠŸèƒ½
"""

import numpy as np
import pygame
import torch
import gym
from gym import spaces
import cv2

# åŠ¨æ€å¯¼å…¥GATæ¨¡å‹
import importlib.util
import os

def _import_gat_model():
    """åŠ¨æ€å¯¼å…¥GATæ¨¡å‹"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    gat_model_path = os.path.join(current_dir, 'gat_model_top.py')

    spec = importlib.util.spec_from_file_location("gat_model_top", gat_model_path)
    gat_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(gat_module)

    return gat_module.UAVAttentionNetwork, gat_module.create_adjacency_matrices

UAVAttentionNetwork, create_adjacency_matrices = _import_gat_model()

# ç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦å¤æ‚çš„é…ç½®å¯¼å…¥


class UAVEnv(gym.Env):
    """ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
    
    def __init__(self,
                 render_mode=None,
                 experiment_type='normal',
                 num_agents=6,
                 num_targets=10,
                 max_steps=200,
                 min_active_agents=3,
                 max_active_agents=None):
        
        # åŸºç¡€å‚æ•°è®¾ç½®
        self.num_agents = num_agents
        self.num_targets = num_targets
        self.world_size = 1.0
        self.max_steps = max_steps
        self.experiment_type = experiment_type
        self.render_mode = render_mode
        
        # ç‰©ç†å‚æ•°
        self.max_speed = 0.1
        self.communication_range = 0.3
        self.sensing_range = 0.2
        
        # æ‹“æ‰‘å‚æ•°
        self.min_active_agents = min_active_agents
        self.max_active_agents = max_active_agents or num_agents
        
        # å¥–åŠ±æƒé‡ï¼ˆä¿æŒåŸæœ‰è®¾ç½®ï¼‰
        self.coverage_weight = 3.5
        self.connectivity_weight = 2.0
        self.boundary_weight = 1.0
        self.stability_weight = 1.5
        
        # æ¦‚ç‡è®¾ç½®ï¼ˆç”¨äºprobabilisticæ¨¡å¼ï¼‰
        self.normal_probability = 0.80
        self.loss_probability = 0.15
        self.addition_probability = 0.05
        
        # çŠ¶æ€å˜é‡
        self.curr_step = 0
        self.active_agents = list(range(self.num_agents))
        self.agents = [f"agent_{i}" for i in range(self.num_agents)]
        
        # ä½ç½®å’Œé€Ÿåº¦
        self.agent_pos = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.target_pos = np.zeros((self.num_targets, 2), dtype=np.float32)
        
        # Episodeè®¡åˆ’
        self.episode_plan = {
            'type': 'normal',
            'trigger_step': None,
            'executed': False
        }

        # GATç¼“å­˜ä¼˜åŒ– - å‡å°‘CPU-GPUä¼ è¾“
        self.gat_cache = {
            'features': None,
            'last_update_step': -1,
            'update_interval': 3  # æ¯3æ­¥æ›´æ–°ä¸€æ¬¡GATç‰¹å¾
        }
        
        # GATç½‘ç»œ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gat_model = UAVAttentionNetwork(
            uav_features=4,      # UAVç‰¹å¾ç»´åº¦ï¼šä½ç½®(2) + é€Ÿåº¦(2)
            target_features=2,   # ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼šä½ç½®(2)
            hidden_size=64,      # éšè—å±‚å¤§å°
            heads=4,             # æ³¨æ„åŠ›å¤´æ•°
            dropout=0.1,         # dropoutç‡
            device=self.device
        )
        
        # è®­ç»ƒæ¨¡å¼æ ‡å¿—
        self.training = False
        
        # è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
        self._setup_spaces()
        
        # æ¸²æŸ“ç›¸å…³
        self.screen = None
        self.clock = None
        self.screen_size = 700
        
        # å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)
        
    def _setup_spaces(self):
        """è®¾ç½®è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ - ä¿æŒåŸæœ‰æ ¼å¼"""
        # è§‚å¯Ÿç»´åº¦è®¡ç®—ï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
        obs_dim = (
            4 +                           # åŸºç¡€çŠ¶æ€ (ä½ç½®+é€Ÿåº¦)
            2 * self.num_targets +       # ç›®æ ‡ç›¸å¯¹ä½ç½®
            2 * (self.num_agents - 1) +  # é‚»å±…ä¿¡æ¯
            32 +                         # GATç‰¹å¾
            3                            # æ‹“æ‰‘ä¿¡æ¯
        )
        
        # ä¿æŒåŸæœ‰çš„å­—å…¸æ ¼å¼
        self.observation_spaces = {
            agent: spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
            for agent in self.agents
        }
        
        self.action_spaces = {
            agent: spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)
            for agent in self.agents
        }
    
    def reset(self, seed=None):
        """é‡ç½®ç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
        if seed is not None:
            np.random.seed(seed)
        
        self.curr_step = 0
        self.active_agents = list(range(self.num_agents))
        
        # éšæœºåˆå§‹åŒ–ä½ç½®
        self.agent_pos = np.random.uniform(-self.world_size, self.world_size, 
                                         (self.num_agents, 2)).astype(np.float32)
        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.target_pos = np.random.uniform(-self.world_size, self.world_size, 
                                          (self.num_targets, 2)).astype(np.float32)
        
        # é‡ç½®å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # é‡ç½®GATç¼“å­˜
        self.gat_cache['features'] = None
        self.gat_cache['last_update_step'] = -1

        # åˆ¶å®šepisodeè®¡åˆ’
        self._plan_episode()
        
        # è¿”å›åŸæœ‰æ ¼å¼çš„è§‚å¯Ÿ
        obs_list = self._get_obs()
        return {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}, {}
    
    def _plan_episode(self):
        """åˆ¶å®šepisodeæ‹“æ‰‘å˜åŒ–è®¡åˆ’"""
        if self.experiment_type != 'probabilistic':
            self.episode_plan = {'type': 'normal', 'trigger_step': None, 'executed': False}
            return
        
        # æ ¹æ®æ¦‚ç‡å†³å®šepisodeç±»å‹
        rand = np.random.random()
        
        if rand < self.normal_probability:
            episode_type = 'normal'
            trigger_step = None
        elif rand < self.normal_probability + self.loss_probability:
            episode_type = 'loss'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        else:
            episode_type = 'addition'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        
        self.episode_plan = {
            'type': episode_type,
            'trigger_step': trigger_step,
            'executed': False
        }
        
        print(f"ğŸ“‹ Episodeè®¡åˆ’: {episode_type}" + 
              (f" (ç¬¬{trigger_step}æ­¥è§¦å‘)" if trigger_step else ""))
    
    def step(self, actions):
        """æ‰§è¡Œä¸€æ­¥ - ä¿æŒåŸæœ‰æ¥å£"""
        self.curr_step += 1
        
        # æ‰§è¡ŒåŠ¨ä½œï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        for i, agent in enumerate(self.agents):
            if i in self.active_agents and agent in actions:
                action = np.array(actions[agent], dtype=np.float32)
                action = np.clip(action, -1.0, 1.0)
                
                # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
                self.agent_vel[i] = action * self.max_speed
                self.agent_pos[i] += self.agent_vel[i]
                
                # è¾¹ç•Œå¤„ç†
                self.agent_pos[i] = np.clip(self.agent_pos[i], 
                                          -self.world_size, self.world_size)
                
                # è®°å½•åŠ¨ä½œ
                self.prev_actions[i] = action
        
        # æ£€æŸ¥æ‹“æ‰‘å˜åŒ–
        self._check_topology_change()
        
        # è®¡ç®—å¥–åŠ±ï¼ˆä¿æŒåŸæœ‰å¤æ‚åº¦ï¼‰
        rewards = self._compute_rewards()
        
        # æ£€æŸ¥ç»“æŸæ¡ä»¶
        dones = {agent: self.curr_step >= self.max_steps for agent in self.agents}
        truncated = dones.copy()
        
        # è·å–è§‚å¯Ÿ
        obs_list = self._get_obs()
        observations = {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}
        
        return observations, rewards, dones, truncated, {}
    
    def _check_topology_change(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œæ‹“æ‰‘å˜åŒ–"""
        if (self.episode_plan['type'] != 'normal' and 
            not self.episode_plan['executed'] and 
            self.curr_step >= self.episode_plan['trigger_step']):
            
            if self.episode_plan['type'] == 'loss':
                success = self._execute_uav_loss()
            elif self.episode_plan['type'] == 'addition':
                success = self._execute_uav_addition()
            else:
                success = False
            
            if success:
                self.episode_plan['executed'] = True
                print(f"ğŸ¯ æ‰§è¡Œè®¡åˆ’: {self.episode_plan['type']} (ç¬¬{self.curr_step}æ­¥)")
    
    def _execute_uav_loss(self):
        """æ‰§è¡ŒUAVæŸå¤±"""
        if len(self.active_agents) <= self.min_active_agents:
            return False
        
        lost_uav = np.random.choice(self.active_agents)
        self.active_agents.remove(lost_uav)
        
        print(f"UAV {lost_uav} å·²å¤±æ•ˆï¼Œå½“å‰ä½ç½®: {self.agent_pos[lost_uav]}")
        print(f"[å®éªŒæ¨¡å¼: UAVæŸå¤±] Step {self.curr_step}: UAV {lost_uav} å¤±æ•ˆ")
        return True
    
    def _execute_uav_addition(self):
        """æ‰§è¡ŒUAVæ·»åŠ """
        if len(self.active_agents) >= self.max_active_agents:
            return False
        
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if not inactive_uavs:
            return False
        
        new_uav = np.random.choice(inactive_uavs)
        self.active_agents.append(new_uav)
        
        # é‡æ–°åˆå§‹åŒ–ä½ç½®
        self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
        self.agent_vel[new_uav] = np.zeros(2)
        
        print(f"[å®éªŒæ¨¡å¼: UAVæ·»åŠ ] Step {self.curr_step}: æ·»åŠ æ–°UAV {new_uav}")
        return True

    def _get_obs(self):
        """è·å–è§‚å¯Ÿ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘é‡å¤è®¡ç®—"""
        # è®¡ç®—GATç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        gat_features = self._compute_gat_features()

        # é¢„è®¡ç®—å…¬å…±æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
        active_agents_ratio = len(self.active_agents) / self.num_agents
        time_progress = self.curr_step / self.max_steps

        # æ‰¹é‡è®¡ç®—ç›®æ ‡ç›¸å¯¹ä½ç½®
        target_relative_all = []
        for i in range(self.num_agents):
            target_relative = (self.target_pos - self.agent_pos[i]).flatten()
            target_relative_all.append(target_relative)

        # æ‰¹é‡è®¡ç®—é‚»å±…ç›¸å¯¹ä½ç½®
        neighbor_relative_all = []
        for i in range(self.num_agents):
            neighbor_relative = []
            for j in range(self.num_agents):
                if j != i:
                    if j in self.active_agents:
                        relative_pos = self.agent_pos[j] - self.agent_pos[i]
                        neighbor_relative.extend(relative_pos)
                    else:
                        neighbor_relative.extend([0.0, 0.0])
            neighbor_relative_all.append(neighbor_relative)

        # æ‰¹é‡è½¬æ¢GATç‰¹å¾
        gat_features_np = []
        for i in range(self.num_agents):
            if i < len(gat_features):
                gat_feat = gat_features[i]
                if torch.is_tensor(gat_feat):
                    if gat_feat.requires_grad:
                        gat_feat = gat_feat.detach().cpu().numpy()
                    else:
                        gat_feat = gat_feat.cpu().numpy()
                gat_features_np.append(gat_feat)
            else:
                gat_features_np.append(np.zeros(32, dtype=np.float32))

        # ç»„è£…è§‚å¯Ÿ
        obs_list = []
        for i in range(self.num_agents):
            obs_parts = np.concatenate([
                self.agent_pos[i],                    # ä½ç½®
                self.agent_vel[i],                    # é€Ÿåº¦
                target_relative_all[i],               # ç›®æ ‡ç›¸å¯¹ä½ç½®
                neighbor_relative_all[i],             # é‚»å±…ä¿¡æ¯
                gat_features_np[i],                   # GATç‰¹å¾
                [active_agents_ratio,                 # æ´»è·ƒæ¯”ä¾‹
                 float(i in self.active_agents),      # è‡ªèº«çŠ¶æ€
                 time_progress]                       # æ—¶é—´è¿›åº¦
            ]).astype(np.float32)

            obs_list.append(obs_parts)

        return obs_list

    def _compute_gat_features(self):
        """è®¡ç®—GATç‰¹å¾ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘CPU-GPUä¼ è¾“"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°GATç‰¹å¾ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰
        if (self.gat_cache['features'] is not None and
            self.curr_step - self.gat_cache['last_update_step'] < self.gat_cache['update_interval']):
            return self.gat_cache['features']

        # å‡†å¤‡è¾“å…¥æ•°æ® - æ‰¹é‡è½¬æ¢å‡å°‘ä¼ è¾“æ¬¡æ•°
        uav_features = np.concatenate([self.agent_pos, self.agent_vel], axis=1)

        # ä¸€æ¬¡æ€§ä¼ è¾“æ‰€æœ‰æ•°æ®åˆ°GPU
        with torch.no_grad():
            uav_tensor = torch.FloatTensor(uav_features).to(self.device, non_blocking=True)
            target_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)
            uav_pos_tensor = torch.FloatTensor(self.agent_pos).to(self.device, non_blocking=True)
            target_pos_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)

        # åˆ›å»ºé‚»æ¥çŸ©é˜µ
        uav_adj, uav_target_adj = create_adjacency_matrices(
            uav_pos_tensor, target_pos_tensor,
            self.communication_range, self.sensing_range,
            active_uavs=self.active_agents
        )

        # GATå‰å‘ä¼ æ’­
        with torch.set_grad_enabled(self.training):
            gat_features = self.gat_model(uav_tensor, target_tensor,
                                        uav_adj, uav_target_adj,
                                        active_agents=self.active_agents)

        if not self.training:
            gat_features = gat_features.detach()

        # æ›´æ–°ç¼“å­˜
        self.gat_cache['features'] = gat_features
        self.gat_cache['last_update_step'] = self.curr_step

        return gat_features

    def _compute_rewards(self):
        """è®¡ç®—å¥–åŠ± - ä¿æŒåŸæœ‰å¤æ‚åº¦"""
        rewards = {}

        # è®¡ç®—è¦†ç›–ç‡
        coverage_rate, _, _, _ = self.calculate_coverage_complete()

        # è®¡ç®—è¿é€šæ€§
        connectivity_reward = self._compute_connectivity_reward()

        # è®¡ç®—ç¨³å®šæ€§
        stability_reward = self._compute_stability_reward()

        # åŸºç¡€å¥–åŠ±
        base_reward = (
            self.coverage_weight * coverage_rate +
            self.connectivity_weight * connectivity_reward +
            self.stability_weight * stability_reward
        )

        for i, agent in enumerate(self.agents):
            if i in self.active_agents:
                reward = base_reward

                # è¾¹ç•Œæƒ©ç½š
                if np.any(np.abs(self.agent_pos[i]) > self.world_size - 0.1):
                    reward -= self.boundary_weight

                rewards[agent] = reward
            else:
                rewards[agent] = 0.0

        return rewards

    def calculate_coverage_complete(self):
        """è®¡ç®—å®Œæ•´è¦†ç›–ç‡ä¿¡æ¯ - ä¿æŒåŸæœ‰æ¥å£"""
        covered_targets = 0
        for target_pos in self.target_pos:
            for agent_idx in self.active_agents:
                distance = np.linalg.norm(target_pos - self.agent_pos[agent_idx])
                if distance <= self.sensing_range:
                    covered_targets += 1
                    break

        coverage_rate = covered_targets / self.num_targets if self.num_targets > 0 else 0.0
        is_fully_connected = True  # ç®€åŒ–å®ç°
        max_coverage_rate = coverage_rate
        unconnected_uav = 0

        return coverage_rate, is_fully_connected, max_coverage_rate, unconnected_uav

    def _compute_connectivity_reward(self):
        """è®¡ç®—è¿é€šæ€§å¥–åŠ±"""
        if len(self.active_agents) <= 1:
            return 1.0

        connected_count = 0
        for i in self.active_agents:
            for j in self.active_agents:
                if i != j:
                    distance = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                    if distance <= self.communication_range:
                        connected_count += 1
                        break

        return connected_count / len(self.active_agents)

    def _compute_stability_reward(self):
        """è®¡ç®—ç¨³å®šæ€§å¥–åŠ±"""
        # ç®€åŒ–çš„ç¨³å®šæ€§è®¡ç®—
        if len(self.coverage_history) < 2:
            return 0.0

        recent_coverage = self.coverage_history[-10:] if len(self.coverage_history) >= 10 else self.coverage_history
        if len(recent_coverage) < 2:
            return 0.0

        stability = 1.0 - np.std(recent_coverage)
        return max(0.0, stability)

    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
        if mode == 'rgb_array':
            return self._render_rgb_array()
        elif mode == 'human':
            return self._render_human()
        else:
            return None

    def _render_human(self):
        """äººç±»å¯è§†åŒ–æ¸²æŸ“"""
        if self.screen is None:
            pygame.init()
            pygame.display.init()
            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))
            self.clock = pygame.time.Clock()

        # æ¸…å±
        self.screen.fill((255, 255, 255))

        # ç»˜åˆ¶ç›®æ ‡
        for target_pos in self.target_pos:
            screen_pos = self._world_to_screen(target_pos)
            pygame.draw.circle(self.screen, (255, 0, 0), screen_pos, 8)

        # ç»˜åˆ¶UAV
        for i in self.active_agents:
            screen_pos = self._world_to_screen(self.agent_pos[i])
            color = (0, 0, 255) if i in self.active_agents else (128, 128, 128)
            pygame.draw.circle(self.screen, color, screen_pos, 12)

            # ç»˜åˆ¶æ„ŸçŸ¥èŒƒå›´
            sensing_radius = int(self.sensing_range * self.screen_size / (2 * self.world_size))
            pygame.draw.circle(self.screen, (0, 255, 0), screen_pos, sensing_radius, 1)

        # æ˜¾ç¤ºå®éªŒä¿¡æ¯
        font = pygame.font.Font(None, 24)
        experiment_text = f"å®éªŒç±»å‹: {self.experiment_type}"
        text_surface = font.render(experiment_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 10))

        step_text = f"æ­¥æ•°: {self.curr_step}/{self.max_steps}"
        text_surface = font.render(step_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 35))

        uav_text = f"æ´»è·ƒUAV: {len(self.active_agents)}/{self.num_agents}"
        text_surface = font.render(uav_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 60))

        pygame.display.flip()
        self.clock.tick(60)

    def _render_rgb_array(self):
        """è¿”å›RGBæ•°ç»„"""
        if self.screen is None:
            self._render_human()

        # è·å–å±å¹•å†…å®¹
        rgb_array = pygame.surfarray.array3d(self.screen)
        rgb_array = np.transpose(rgb_array, (1, 0, 2))
        return rgb_array

    def _world_to_screen(self, world_pos):
        """ä¸–ç•Œåæ ‡è½¬å±å¹•åæ ‡"""
        x = int((world_pos[0] + self.world_size) / (2 * self.world_size) * self.screen_size)
        y = int((world_pos[1] + self.world_size) / (2 * self.world_size) * self.screen_size)
        return (x, y)

    # GATç›¸å…³æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_gat_parameters(self):
        """è·å–GATå‚æ•°"""
        return self.gat_model.parameters()

    def save_gat_model(self, path):
        """ä¿å­˜GATæ¨¡å‹"""
        torch.save(self.gat_model.state_dict(), path)

    def load_gat_model(self, path):
        """åŠ è½½GATæ¨¡å‹"""
        self.gat_model.load_state_dict(torch.load(path, map_location=self.device))

    # å…¼å®¹æ€§æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_observation_space(self, agent):
        """è·å–è§‚å¯Ÿç©ºé—´"""
        return self.observation_spaces[agent]

    def get_action_space(self, agent):
        """è·å–åŠ¨ä½œç©ºé—´"""
        return self.action_spaces[agent]

    def fail_uav(self, uav_idx):
        """ä½¿UAVå¤±æ•ˆ - å…¼å®¹æ€§æ–¹æ³•"""
        if uav_idx in self.active_agents:
            self.active_agents.remove(uav_idx)
            return True
        return False

    def add_uav(self):
        """æ·»åŠ UAV - å…¼å®¹æ€§æ–¹æ³•"""
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if inactive_uavs:
            new_uav = inactive_uavs[0]
            self.active_agents.append(new_uav)
            self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
            self.agent_vel[new_uav] = np.zeros(2)
            return new_uav
        return None

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.screen is not None:
            pygame.display.quit()
            pygame.quit()
            self.screen = None
            self.clock = None
