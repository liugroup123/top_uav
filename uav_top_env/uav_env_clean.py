"""
ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£å’ŒåŠŸèƒ½
åŸºäºåŸç‰ˆuav_env_top.pyï¼Œåˆ é™¤å†—ä½™ä»£ç ï¼Œä¿ç•™æ ¸å¿ƒåŠŸèƒ½
"""

import numpy as np
import pygame
import torch
import gym
from gym import spaces
import cv2
import networkx as nx

# åŠ¨æ€å¯¼å…¥GATæ¨¡å‹
import importlib.util
import os

def _import_gat_model():
    """åŠ¨æ€å¯¼å…¥GATæ¨¡å‹"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    gat_model_path = os.path.join(current_dir, 'gat_model_top.py')

    spec = importlib.util.spec_from_file_location("gat_model_top", gat_model_path)
    gat_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(gat_module)

    return gat_module.UAVAttentionNetwork, gat_module.create_adjacency_matrices

UAVAttentionNetwork, create_adjacency_matrices = _import_gat_model()

# ç®€åŒ–ç‰ˆæœ¬ä¸éœ€è¦å¤æ‚çš„é…ç½®å¯¼å…¥


class UAVEnv(gym.Env):
    """ç®€åŒ–çš„æ‹“æ‰‘UAVç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
    
    def __init__(self,
                 render_mode=None,
                 experiment_type='normal',
                 num_agents=6,
                 num_targets=10,
                 max_steps=200,
                 min_active_agents=3,
                 max_active_agents=None):
        
        # åŸºç¡€å‚æ•°è®¾ç½®
        self.num_agents = num_agents
        self.num_targets = num_targets
        self.world_size = 1.0
        self.max_steps = max_steps
        self.experiment_type = experiment_type
        self.render_mode = render_mode
        self.curr_step = 0
        self.max_coverage_rate = 0.0
        
        # ç‰©ç†å‚æ•°
        self.max_speed = 2.0
        self.communication_range = 0.8
        self.coverage_radius = 0.4
        self.dt = 0.1
        
        # æ‹“æ‰‘å‚æ•°
        self.min_active_agents = min_active_agents
        self.max_active_agents = max_active_agents or num_agents
        
        # æ–°çš„å¥–åŠ±æƒé‡ (åŸºäºåŠ¨æ€é¿éšœå®éªŒè°ƒæ•´)
        self.connectivity_weight = 1.0      # è¿é€šæ€§æƒé‡
        self.coverage_weight = 20.0         # è¦†ç›–æƒé‡ (é™ä½)
        self.stability_weight = 0.5         # ç¨³å®šæ€§æƒé‡
        self.topology_weight = 5.0          # æ‹“æ‰‘é€‚åº”æƒé‡ (æ–°å¢)
        self.boundary_weight = 1.0          # ä¿ç•™è¾¹ç•Œæƒé‡
        
        # æ‹“æ‰‘å®éªŒæ¦‚ç‡
        self.normal_probability = 0.60
        self.loss_probability = 0.25
        self.addition_probability = 0.15
        
        # çŠ¶æ€å˜é‡
        self.curr_step = 0
        self.active_agents = list(range(self.num_agents))
        self.agents = [f"agent_{i}" for i in range(self.num_agents)]
        
        # ä½ç½®å’Œé€Ÿåº¦
        self.agent_pos = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)
        self.target_pos = np.zeros((self.num_targets, 2), dtype=np.float32)
        
        # Episodeè®¡åˆ’
        self.episode_plan = {
            'type': 'normal',
            'trigger_step': None,
            'executed': False
        }

        # GATç¼“å­˜
        self.gat_cache = {
            'features': None,
            'last_update_step': -1,
            'update_interval': 3
        }

        # é€Ÿåº¦é™åˆ¶å‚æ•°
        self.max_base_speed = 1.0
        self.connectivity_speed_factor = 0.5
        self.min_speed_limit = 0.2
        self.epsilon = 0.1
        self.speed_violation_penalty = -5.0

        # è¿æ¥æ€§å†å²è®°å½•
        self.connectivity_history = []
        self.speed_limits_history = []
        
        # GATç½‘ç»œ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gat_model = UAVAttentionNetwork(
            uav_features=4,      # UAVç‰¹å¾ç»´åº¦ï¼šä½ç½®(2) + é€Ÿåº¦(2)
            target_features=2,   # ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼šä½ç½®(2)
            hidden_size=64,      # éšè—å±‚å¤§å°
            heads=4,             # æ³¨æ„åŠ›å¤´æ•°
            dropout=0.1,         # dropoutç‡
            device=self.device
        )
        
        # è®­ç»ƒæ¨¡å¼æ ‡å¿—
        self.training = False
        
        # è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
        self._setup_spaces()
        
        # æ¸²æŸ“ç›¸å…³ - æé«˜åˆ†è¾¨ç‡ä»¥æ”¹å–„è§†é¢‘ç”»è´¨
        self.screen = None
        self.clock = None
        self.width, self.height = (800, 800)  # ä»700x700æå‡åˆ°1200x1200
        self.metadata = {"render_fps": 60}
        self.font = None
        
        # å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åŠ é€Ÿåº¦æ§åˆ¶å‚æ•°
        self.max_acceleration = 4.0  # æœ€å¤§åŠ é€Ÿåº¦
        self.damping_factor = 0.95   # é˜»å°¼ç³»æ•°ï¼ˆå¯é€‰ï¼‰

        # ç¨³å®šæ€§å¥–åŠ±å‚æ•° (ç§»æ¤è‡ªåŠ¨æ€é¿éšœå®éªŒ)
        self.training_step = 0
        self.initial_threshold = 0.6
        self.threshold_increase_rate = 0.0001
        self.max_threshold = 0.95
        self.stability_bonus_value = 50.0
        self.speed_tolerance = 0.1

        # è¦†ç›–å¥–åŠ±å‚æ•°
        self.covered_targets = set()  # å·²è¦†ç›–çš„ç›®æ ‡é›†åˆ
        self.unique_coverage_weight = 30.0
        
    def _setup_spaces(self):
        """è®¾ç½®è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ - ä¿æŒåŸæœ‰æ ¼å¼"""
        # è§‚å¯Ÿç»´åº¦è®¡ç®—ï¼ˆä¸åŸç‰ˆä¸€è‡´ï¼‰
        obs_dim = (
            4 +                           # åŸºç¡€çŠ¶æ€ (ä½ç½®+é€Ÿåº¦)
            2 * self.num_targets +       # ç›®æ ‡ç›¸å¯¹ä½ç½®
            2 * (self.num_agents - 1) +  # é‚»å±…ä¿¡æ¯
            32 +                         # GATç‰¹å¾
            3                            # æ‹“æ‰‘ä¿¡æ¯
        )
        
        # ä¿æŒåŸæœ‰çš„å­—å…¸æ ¼å¼
        self.observation_spaces = {
            agent: spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
            for agent in self.agents
        }
        
        self.action_spaces = {
            agent: spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)
            for agent in self.agents
        }
    
    def reset(self, seed=None):
        """é‡ç½®ç¯å¢ƒ - ä¿æŒåŸæœ‰æ¥å£"""
        if seed is not None:
            np.random.seed(seed)
        
        self.curr_step = 0
        self.max_coverage_rate = 0.0
        self.active_agents = list(range(self.num_agents))

        # é‡ç½®å¥–åŠ±ç›¸å…³å‚æ•°
        self.covered_targets = set()  # é‡ç½®å·²è¦†ç›–ç›®æ ‡é›†åˆ
        
        # åˆå§‹åŒ–UAVä½ç½®ï¼ˆä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼šåº•éƒ¨æ’åˆ—ï¼‰
        self.agent_pos = []
        bottom_y = -self.world_size + 0.15
        spacing = 2 * self.world_size / (self.num_agents + 1)
        for i in range(self.num_agents):
            x = -self.world_size + (i + 1) * spacing
            self.agent_pos.append([x, bottom_y])
        self.agent_pos = np.array(self.agent_pos, dtype=np.float32)

        self.agent_vel = np.zeros((self.num_agents, 2), dtype=np.float32)

        # åˆå§‹åŒ–ç›®æ ‡ä½ç½®ï¼ˆä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼š0.85èŒƒå›´å†…éšæœºåˆ†å¸ƒï¼‰
        self.target_pos = np.random.uniform(-self.world_size*0.85, self.world_size*0.85,
                                          (self.num_targets, 2)).astype(np.float32)
        
        # é‡ç½®å†å²è®°å½•
        self.coverage_history = []
        self.prev_actions = np.zeros((self.num_agents, 2), dtype=np.float32)

        # é‡ç½®GATç¼“å­˜
        self.gat_cache['features'] = None
        self.gat_cache['last_update_step'] = -1

        # é‡ç½®é€Ÿåº¦é™åˆ¶å†å²
        self.connectivity_history = []
        self.speed_limits_history = []

        # åˆ¶å®šepisodeè®¡åˆ’
        self._plan_episode()
        
        # è¿”å›åŸæœ‰æ ¼å¼çš„è§‚å¯Ÿ
        obs_list = self._get_obs()
        return {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}, {}
    
    def _plan_episode(self):
        """åˆ¶å®šepisodeæ‹“æ‰‘å˜åŒ–è®¡åˆ’"""
        if self.experiment_type != 'probabilistic':
            self.episode_plan = {'type': 'normal', 'trigger_step': None, 'executed': False}
            return
        
        # æ ¹æ®æ¦‚ç‡å†³å®šepisodeç±»å‹
        rand = np.random.random()
        
        if rand < self.normal_probability:
            episode_type = 'normal'
            trigger_step = None
        elif rand < self.normal_probability + self.loss_probability:
            episode_type = 'loss'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        else:
            episode_type = 'addition'
            trigger_step = int(self.max_steps * (0.3 + 0.4 * np.random.random()))
        
        self.episode_plan = {
            'type': episode_type,
            'trigger_step': trigger_step,
            'executed': False
        }
        
        print(f"ğŸ“‹ Episodeè®¡åˆ’: {episode_type}" + 
              (f" (ç¬¬{trigger_step}æ­¥è§¦å‘)" if trigger_step else ""))
    
    def step(self, actions):
        """æ‰§è¡Œä¸€æ­¥ - ä½¿ç”¨åŠ é€Ÿåº¦æ§åˆ¶"""
        self.curr_step += 1

        # è®¡ç®—å½“å‰æ­¥çš„åŠ¨æ€é€Ÿåº¦é™åˆ¶
        speed_limits = self._compute_connectivity_based_speed_limits()
        self.speed_limits_history.append(speed_limits.copy())

        # è®°å½•é€Ÿåº¦è¿è§„æƒ…å†µ
        speed_violations = {}

        # æ‰§è¡ŒåŠ¨ä½œï¼ˆåŠ é€Ÿåº¦æ§åˆ¶ï¼‰
        for i, agent in enumerate(self.agents):
            if i in self.active_agents and agent in actions:
                action = np.array(actions[agent], dtype=np.float32)
                action = np.clip(action, -1.0, 1.0)

                # æ›´æ–°æ™ºèƒ½ä½“é€Ÿåº¦å’Œä½ç½®
                self._update_agent_dynamics(i, action, speed_limits[i], speed_violations, agent)

                # è®°å½•åŸå§‹åŠ¨ä½œ
                self.prev_actions[i] = action
        
        # æ£€æŸ¥æ‹“æ‰‘å˜åŒ–
        self._check_topology_change()

        # è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«é€Ÿåº¦é™åˆ¶å¥–åŠ±ï¼‰
        rewards = self._compute_rewards(speed_violations)
        
        # æ£€æŸ¥ç»“æŸæ¡ä»¶
        dones = {agent: self.curr_step >= self.max_steps for agent in self.agents}
        truncated = dones.copy()
        
        # è·å–è§‚å¯Ÿ
        obs_list = self._get_obs()
        observations = {f"agent_{i}": obs_list[i] for i in range(self.num_agents)}
        
        return observations, rewards, dones, truncated, {}
    
    def _check_topology_change(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œæ‹“æ‰‘å˜åŒ–"""
        if (self.episode_plan['type'] != 'normal' and 
            not self.episode_plan['executed'] and 
            self.curr_step >= self.episode_plan['trigger_step']):
            
            if self.episode_plan['type'] == 'loss':
                success = self._execute_uav_loss()
            elif self.episode_plan['type'] == 'addition':
                success = self._execute_uav_addition()
            else:
                success = False
            
            if success:
                self.episode_plan['executed'] = True
                print(f"ğŸ¯ æ‰§è¡Œè®¡åˆ’: {self.episode_plan['type']} (ç¬¬{self.curr_step}æ­¥)")
    
    def _execute_uav_loss(self):
        """æ‰§è¡ŒUAVæŸå¤±"""
        if len(self.active_agents) <= self.min_active_agents:
            return False
        
        lost_uav = np.random.choice(self.active_agents)
        self.active_agents.remove(lost_uav)
        
        print(f"UAV {lost_uav} å·²å¤±æ•ˆï¼Œå½“å‰ä½ç½®: {self.agent_pos[lost_uav]}")
        print(f"[å®éªŒæ¨¡å¼: UAVæŸå¤±] Step {self.curr_step}: UAV {lost_uav} å¤±æ•ˆ")
        return True
    
    def _execute_uav_addition(self):
        """æ‰§è¡ŒUAVæ·»åŠ """
        if len(self.active_agents) >= self.max_active_agents:
            return False
        
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if not inactive_uavs:
            return False
        
        new_uav = np.random.choice(inactive_uavs)
        self.active_agents.append(new_uav)
        
        # é‡æ–°åˆå§‹åŒ–ä½ç½®
        self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
        self.agent_vel[new_uav] = np.zeros(2)
        
        print(f"[å®éªŒæ¨¡å¼: UAVæ·»åŠ ] Step {self.curr_step}: æ·»åŠ æ–°UAV {new_uav}")
        return True

    def _get_obs(self):
        """è·å–è§‚å¯Ÿ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘é‡å¤è®¡ç®—"""
        # è®¡ç®—GATç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        gat_features = self._compute_gat_features()

        # é¢„è®¡ç®—å…¬å…±æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
        active_agents_ratio = len(self.active_agents) / self.num_agents
        time_progress = self.curr_step / self.max_steps

        # æ‰¹é‡è®¡ç®—ç›®æ ‡ç›¸å¯¹ä½ç½®
        target_relative_all = []
        for i in range(self.num_agents):
            target_relative = (self.target_pos - self.agent_pos[i]).flatten()
            target_relative_all.append(target_relative)

        # æ‰¹é‡è®¡ç®—é‚»å±…ç›¸å¯¹ä½ç½®
        neighbor_relative_all = []
        for i in range(self.num_agents):
            neighbor_relative = []
            for j in range(self.num_agents):
                if j != i:
                    if j in self.active_agents:
                        relative_pos = self.agent_pos[j] - self.agent_pos[i]
                        neighbor_relative.extend(relative_pos)
                    else:
                        neighbor_relative.extend([0.0, 0.0])
            neighbor_relative_all.append(neighbor_relative)

        # æ‰¹é‡è½¬æ¢GATç‰¹å¾
        gat_features_np = []
        for i in range(self.num_agents):
            if i < len(gat_features):
                gat_feat = gat_features[i]
                if torch.is_tensor(gat_feat):
                    if gat_feat.requires_grad:
                        gat_feat = gat_feat.detach().cpu().numpy()
                    else:
                        gat_feat = gat_feat.cpu().numpy()
                gat_features_np.append(gat_feat)
            else:
                gat_features_np.append(np.zeros(32, dtype=np.float32))

        # ç»„è£…è§‚å¯Ÿ
        obs_list = []
        for i in range(self.num_agents):
            obs_parts = np.concatenate([
                self.agent_pos[i],                    # ä½ç½®
                self.agent_vel[i],                    # é€Ÿåº¦
                target_relative_all[i],               # ç›®æ ‡ç›¸å¯¹ä½ç½®
                neighbor_relative_all[i],             # é‚»å±…ä¿¡æ¯
                gat_features_np[i],                   # GATç‰¹å¾
                [active_agents_ratio,                 # æ´»è·ƒæ¯”ä¾‹
                 float(i in self.active_agents),      # è‡ªèº«çŠ¶æ€
                 time_progress]                       # æ—¶é—´è¿›åº¦
            ]).astype(np.float32)

            obs_list.append(obs_parts)

        return obs_list

    def _compute_gat_features(self):
        """è®¡ç®—GATç‰¹å¾ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘CPU-GPUä¼ è¾“"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°GATç‰¹å¾ï¼ˆç¼“å­˜æœºåˆ¶ï¼‰
        if (self.gat_cache['features'] is not None and
            self.curr_step - self.gat_cache['last_update_step'] < self.gat_cache['update_interval']):
            return self.gat_cache['features']

        # å‡†å¤‡è¾“å…¥æ•°æ® - æ‰¹é‡è½¬æ¢å‡å°‘ä¼ è¾“æ¬¡æ•°
        uav_features = np.concatenate([self.agent_pos, self.agent_vel], axis=1)

        # ä¸€æ¬¡æ€§ä¼ è¾“æ‰€æœ‰æ•°æ®åˆ°GPU
        with torch.no_grad():
            uav_tensor = torch.FloatTensor(uav_features).to(self.device, non_blocking=True)
            target_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)
            uav_pos_tensor = torch.FloatTensor(self.agent_pos).to(self.device, non_blocking=True)
            target_pos_tensor = torch.FloatTensor(self.target_pos).to(self.device, non_blocking=True)

        # åˆ›å»ºé‚»æ¥çŸ©é˜µ
        uav_adj, uav_target_adj = create_adjacency_matrices(
            uav_pos_tensor, target_pos_tensor,
            self.communication_range, self.coverage_radius,
            active_uavs=self.active_agents
        )

        # GATå‰å‘ä¼ æ’­
        with torch.set_grad_enabled(self.training):
            gat_features = self.gat_model(uav_tensor, target_tensor,
                                        uav_adj, uav_target_adj,
                                        active_agents=self.active_agents)

        if not self.training:
            gat_features = gat_features.detach()

        # æ›´æ–°ç¼“å­˜
        self.gat_cache['features'] = gat_features
        self.gat_cache['last_update_step'] = self.curr_step

        return gat_features

    def _compute_connectivity_based_speed_limits(self):
        """
        åŸºäºè¿æ¥æ€§è®¡ç®—æ¯ä¸ªUAVçš„åŠ¨æ€é€Ÿåº¦é™åˆ¶
        å®ç°è®ºæ–‡å¼•ç†1çš„è¿æ¥æ€§ä¿æŒçº¦æŸ
        """
        speed_limits = np.full(self.num_agents, self.max_base_speed, dtype=np.float32)

        # è®¡ç®—å½“å‰è¿æ¥æ€§çŸ©é˜µ
        connectivity_matrix = self._compute_connectivity_matrix()

        for i in range(self.num_agents):
            if i not in self.active_agents:
                speed_limits[i] = 0.0
                continue

            # æ‰¾åˆ°å…³é”®é‚»å±…é›†åˆ N_i^c (critical neighbors)
            critical_neighbors = self._find_critical_neighbors(i, connectivity_matrix)

            if len(critical_neighbors) == 0:
                # æ²¡æœ‰å…³é”®é‚»å±…ï¼Œä½¿ç”¨åŸºç¡€é€Ÿåº¦é™åˆ¶
                speed_limits[i] = self.max_base_speed
            else:
                # è®¡ç®—åŸºäºé‚»å±…è·ç¦»çš„é€Ÿåº¦çº¦æŸ
                min_constraint = float('inf')

                for j in critical_neighbors:
                    if j in self.active_agents:
                        # è®¡ç®—åˆ°é‚»å±…jçš„è·ç¦»
                        dist_ij = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])

                        # åŸºäºå¼•ç†1çš„çº¦æŸè®¡ç®—
                        # Îµ_i = min(r_c - d_ij) â‰¤ Îµ
                        epsilon_i = min(self.communication_range - dist_ij, self.epsilon)

                        if epsilon_i > 0:
                            # Î”d_i â‰¤ Îµ_i/2 (å½“N_i^c â‰  âˆ…æ—¶)
                            constraint = epsilon_i / 2.0
                            min_constraint = min(min_constraint, constraint)

                if min_constraint != float('inf'):
                    speed_limits[i] = max(min_constraint, self.min_speed_limit)
                else:
                    speed_limits[i] = self.max_base_speed

        return speed_limits

    def _compute_connectivity_matrix(self):
        """è®¡ç®—è¿æ¥æ€§çŸ©é˜µ"""
        n = self.num_agents
        connectivity_matrix = np.zeros((n, n), dtype=bool)

        for i in range(n):
            for j in range(i+1, n):
                if i in self.active_agents and j in self.active_agents:
                    dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                    if dist <= self.communication_range:
                        connectivity_matrix[i, j] = True
                        connectivity_matrix[j, i] = True

        return connectivity_matrix

    def _find_critical_neighbors(self, agent_id, connectivity_matrix):
        """
        æ‰¾åˆ°agentçš„å…³é”®é‚»å±…é›†åˆ
        å…³é”®é‚»å±…æ˜¯é‚£äº›å¯¹ä¿æŒå…¨å±€è¿é€šæ€§è‡³å…³é‡è¦çš„é‚»å±…
        """
        critical_neighbors = []

        # è·å–å½“å‰é‚»å±…
        current_neighbors = np.where(connectivity_matrix[agent_id])[0]

        for neighbor in current_neighbors:
            if neighbor in self.active_agents:
                # æ£€æŸ¥ç§»é™¤è¿™ä¸ªè¿æ¥æ˜¯å¦ä¼šå½±å“å…¨å±€è¿é€šæ€§
                temp_matrix = connectivity_matrix.copy()
                temp_matrix[agent_id, neighbor] = False
                temp_matrix[neighbor, agent_id] = False

                if not self._is_graph_connected(temp_matrix):
                    critical_neighbors.append(neighbor)

        return critical_neighbors

    def _is_graph_connected(self, adjacency_matrix):
        """æ£€æŸ¥å›¾æ˜¯å¦è¿é€šï¼ˆä½¿ç”¨DFSï¼‰"""
        active_indices = [i for i in range(self.num_agents) if i in self.active_agents]

        if len(active_indices) <= 1:
            return True

        # ä»ç¬¬ä¸€ä¸ªæ´»è·ƒèŠ‚ç‚¹å¼€å§‹DFS
        visited = set()
        stack = [active_indices[0]]

        while stack:
            node = stack.pop()
            if node in visited:
                continue
            visited.add(node)

            # æ·»åŠ æ‰€æœ‰è¿æ¥çš„é‚»å±…
            for neighbor in range(self.num_agents):
                if (neighbor in self.active_agents and
                    neighbor not in visited and
                    adjacency_matrix[node, neighbor]):
                    stack.append(neighbor)

        return len(visited) == len(active_indices)

    def _compute_rewards(self, speed_violations=None):
        """è®¡ç®—å¥–åŠ±"""
        self.training_step += 1
        rewards = {}

        # 1. è®¡ç®—å…¨å±€å¥–åŠ±ç»„ä»¶
        connectivity_reward = self._compute_connectivity_reward_advanced()
        coverage_reward = self._compute_coverage_reward_advanced()
        stability_reward = self._compute_stability_reward_advanced()
        topology_reward = self._compute_topology_adaptation_reward()

        # 2. å…¨å±€å¥–åŠ±
        global_reward = (connectivity_reward * self.connectivity_weight +
                        coverage_reward * self.coverage_weight +
                        stability_reward * self.stability_weight +
                        topology_reward * self.topology_weight)

        # 3. ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…å¥–åŠ±
        for i, agent in enumerate(self.agents):
            if i in self.active_agents:
                # ä¸ªä½“å¥–åŠ± = å…¨å±€å¥–åŠ± + ä¸ªä½“ç‰¹å®šå¥–åŠ±
                individual_reward = self._compute_individual_reward(i)
                boundary_penalty = self._calculate_boundary_penalty(i)

                total_reward = global_reward + individual_reward + boundary_penalty
                rewards[agent] = total_reward / 100.0  # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
            else:
                rewards[agent] = 0.0

        return rewards

    def _compute_connectivity_reward_advanced(self):
        """è®¡ç®—è¿é€šæ€§å¥–åŠ± """
        # æ„å»ºé€šä¿¡å›¾
        G = nx.Graph()
        active_agents = list(self.active_agents)

        # æ·»åŠ èŠ‚ç‚¹
        for i in active_agents:
            G.add_node(i)

        # æ·»åŠ è¾¹ (é€šä¿¡è¿æ¥)
        for i in active_agents:
            for j in active_agents:
                if i >= j:
                    continue
                dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                if dist <= self.communication_range:
                    G.add_edge(i, j)

        # è®¡ç®—ä»£æ•°è¿é€šåº¦
        try:
            if len(active_agents) <= 1:
                Î»2 = 0
            else:
                Î»2 = nx.algebraic_connectivity(G)
        except:
            Î»2 = 0  # å¦‚æœå›¾ä¸è¿é€šï¼Œåˆ™è¿é€šåº¦ä¸º0

        # åˆ†çº§å¥–åŠ± (ç§»æ¤è‡ªåŸå®éªŒ)
        if Î»2 == 0:
            return -3
        elif Î»2 < 0.2:
            return -0.5
        else:
            return 0

    def _compute_coverage_reward_advanced(self):
        """è®¡ç®—è¦†ç›–å¥–åŠ± """
        # 1. è®¡ç®—å”¯ä¸€è¦†ç›–ç‡
        coverage_rate, _, _, _ = self.calculate_coverage_complete()

        # 2. è®¡ç®—å¹³å‡æœ€å°è·ç¦»
        if len(self.active_agents) == 0:
            return 0

        agent_positions = np.array([self.agent_pos[i] for i in self.active_agents])
        target_positions = self.target_pos

        # è®¡ç®—æ¯ä¸ªç›®æ ‡åˆ°æœ€è¿‘UAVçš„è·ç¦»
        distances = np.linalg.norm(agent_positions[:, None, :] - target_positions[None, :, :], axis=2)
        min_distances = np.min(distances, axis=0)
        avg_min_distance = np.mean(min_distances)
        clipped_avg_min_distance = np.clip(avg_min_distance, 0, 15)

        # 3. å¤åˆå¥–åŠ± (è¦†ç›–ç‡^1.5 Ã— å¹³å‡è·ç¦»)
        r_s_d = (coverage_rate ** 1.5) * clipped_avg_min_distance

        # 4. æƒé‡è°ƒæ•´
        k_1 = 35 * len(self.active_agents)
        return k_1 * r_s_d

    def _compute_stability_reward_advanced(self):
        """è®¡ç®—ç¨³å®šæ€§å¥–åŠ± """
        stability_reward = 0
        coverage_rate, _, _, _ = self.calculate_coverage_complete()

        # åŠ¨æ€é˜ˆå€¼è®¡ç®—
        dynamic_threshold = min(self.initial_threshold + self.training_step * self.threshold_increase_rate,
                               self.max_threshold)

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ™ºèƒ½ä½“éƒ½ç¬¦åˆç¨³å®šæ¡ä»¶
        all_agents_stable = True

        if coverage_rate >= dynamic_threshold:
            for i in self.active_agents:
                # è·å–å½“å‰æ™ºèƒ½ä½“çš„é€Ÿåº¦
                current_speed = np.linalg.norm(self.agent_vel[i])

                # æ£€æŸ¥è¯¥æ™ºèƒ½ä½“æ˜¯å¦æ»¡è¶³é€Ÿåº¦æ¡ä»¶
                if current_speed > self.max_speed * self.speed_tolerance:
                    all_agents_stable = False
                    break

            # å¦‚æœæ‰€æœ‰æ™ºèƒ½ä½“éƒ½æ»¡è¶³ç¨³å®šæ¡ä»¶ï¼Œç»™äºˆæ•´ä½“å¥–åŠ±
            if all_agents_stable:
                stability_reward += self.stability_bonus_value

        return stability_reward * 0.5  # è°ƒæ•´æƒé‡

    def _compute_topology_adaptation_reward(self):
        """è®¡ç®—æ‹“æ‰‘é€‚åº”å¥–åŠ±"""
        if not hasattr(self, 'episode_plan'):
            return 0

        # å¦‚æœå‘ç”Ÿäº†æ‹“æ‰‘å˜åŒ–
        if self.episode_plan.get('executed', False):
            coverage_rate, _, _, _ = self.calculate_coverage_complete()

            # æ‹“æ‰‘å˜åŒ–åçš„é€‚åº”æ€§å¥–åŠ±
            if coverage_rate > 0.7:  # å¿«é€Ÿæ¢å¤é«˜è¦†ç›–ç‡
                return 10.0
            elif coverage_rate > 0.5:  # ä¸­ç­‰æ¢å¤
                return 5.0
            else:  # æ¢å¤è¾ƒæ…¢
                return -2.0

        return 0

    def _compute_individual_reward(self, agent_idx):
        """è®¡ç®—ä¸ªä½“å¥–åŠ± """
        reward = 0

        # 1. å”¯ä¸€è¦†ç›–å¥–åŠ±
        unique_coverage_reward = 0.0
        agent_pos = self.agent_pos[agent_idx]

        for target_pos in self.target_pos:
            if np.linalg.norm(agent_pos - target_pos) < self.coverage_radius:
                # æ£€æŸ¥æ˜¯å¦åªæœ‰å½“å‰æ— äººæœºè¦†ç›–è¯¥ç›®æ ‡ç‚¹
                covered_by_others = False
                for other_idx in self.active_agents:
                    if other_idx == agent_idx:
                        continue
                    if np.linalg.norm(self.agent_pos[other_idx] - target_pos) < self.coverage_radius:
                        covered_by_others = True
                        break

                if not covered_by_others:
                    unique_coverage_reward += 1.0

        reward += unique_coverage_reward * self.unique_coverage_weight

        # 2. æ™ºèƒ½ä½“é—´è·ç¦»å¥–åŠ±/æƒ©ç½š (é€‚é…æ‹“æ‰‘ç¯å¢ƒ)
        detection_radius = self.communication_range * 0.5
        min_radius = detection_radius
        max_radius = 2 * detection_radius

        for other_idx in self.active_agents:
            if other_idx == agent_idx:
                continue
            dist = np.linalg.norm(agent_pos - self.agent_pos[other_idx])
            if dist < 0.75 * min_radius:
                reward -= 2  # è¿‡è¿‘æƒ©ç½š
            elif min_radius <= dist <= max_radius:
                reward += 1.0  # é€‚å½“è·ç¦»å¥–åŠ±

        return reward

    def _calculate_boundary_penalty(self, agent_idx):
        """
        è®¡ç®—è¾¹ç•Œæƒ©ç½š - ç§»æ¤å¹¶é€‚é…è‡ªåŠ¨æ€é¿éšœå®éªŒ
        é€‚é…æ‚¨çš„æ‹“æ‰‘UAVç¯å¢ƒï¼Œç®€åŒ–å¹¶è°ƒæ•´å‚æ•°
        """
        penalty = 0.0
        agent_pos = self.agent_pos[agent_idx]

        # é€‚é…å‚æ•° (æ¯”åŸç‰ˆæ¸©å’Œ)
        max_penalty = 50.0           # é™ä½æƒ©ç½šä¸Šé™
        boundary_limit = self.world_size  # ä½¿ç”¨ç¯å¢ƒçš„world_size (1.0)
        safe_margin = 0.1            # å®‰å…¨è¾¹è·
        penalty_factor = 10.0        # é™ä½æƒ©ç½šå› å­
        penalty_exponent = 2.0       # é™ä½æŒ‡æ•°æ–œç‡

        # æ£€æŸ¥æ¯ä¸ªç»´åº¦ (x, y)
        for dim in range(2):
            pos = agent_pos[dim]

            # 1. æ£€æŸ¥æ¥è¿‘è´Ÿè¾¹ç•Œ (pos < -boundary_limit + safe_margin)
            if pos < -boundary_limit + safe_margin:
                distance_to_boundary = (-boundary_limit + safe_margin) - pos
                if distance_to_boundary > 0:
                    # æ¸©å’Œçš„æŒ‡æ•°æƒ©ç½š
                    penalty -= penalty_factor * np.exp(penalty_exponent * distance_to_boundary)
                    penalty = np.clip(penalty, -max_penalty, 0)

            # 2. æ£€æŸ¥è¶…å‡ºè´Ÿè¾¹ç•Œ (pos < -boundary_limit)
            if pos < -boundary_limit:
                distance_outside = -boundary_limit - pos
                # ä¸¥å‰çš„è¶…å‡ºè¾¹ç•Œæƒ©ç½š
                penalty -= penalty_factor * 2 * np.exp(penalty_exponent * distance_outside)
                penalty = np.clip(penalty, -max_penalty * 2, 0)

            # 3. æ£€æŸ¥æ¥è¿‘æ­£è¾¹ç•Œ (pos > boundary_limit - safe_margin)
            if pos > boundary_limit - safe_margin:
                distance_to_boundary = pos - (boundary_limit - safe_margin)
                if distance_to_boundary > 0:
                    # æ¸©å’Œçš„æŒ‡æ•°æƒ©ç½š
                    penalty -= penalty_factor * np.exp(penalty_exponent * distance_to_boundary)
                    penalty = np.clip(penalty, -max_penalty, 0)

            # 4. æ£€æŸ¥è¶…å‡ºæ­£è¾¹ç•Œ (pos > boundary_limit)
            if pos > boundary_limit:
                distance_outside = pos - boundary_limit
                # ä¸¥å‰çš„è¶…å‡ºè¾¹ç•Œæƒ©ç½š
                penalty -= penalty_factor * 2 * np.exp(penalty_exponent * distance_outside)
                penalty = np.clip(penalty, -max_penalty * 2, 0)

        return penalty

    def _compute_speed_compliance_reward(self, speed_violations):
        """è®¡ç®—é€Ÿåº¦åˆè§„å¥–åŠ±"""
        if not speed_violations:
            return 0.0

        total_compliance = 0.0
        active_count = 0

        for agent in self.agents:
            if agent in speed_violations:
                violation = speed_violations[agent]['violation']
                # åˆè§„åº¦ = 1 - (è¿è§„ç¨‹åº¦ / æœ€å¤§å¯èƒ½è¿è§„)
                compliance = max(0.0, 1.0 - violation / self.max_base_speed)
                total_compliance += compliance
                active_count += 1

        return total_compliance / max(active_count, 1)

    def _update_agent_dynamics(self, agent_idx, action, speed_limit, speed_violations, agent_name):
        """æ›´æ–°å•ä¸ªæ™ºèƒ½ä½“çš„åŠ¨åŠ›å­¦çŠ¶æ€ï¼ˆåŠ é€Ÿåº¦æ§åˆ¶ï¼‰"""
        # 1. åŠ¨ä½œè½¬æ¢ä¸ºåŠ é€Ÿåº¦
        acceleration = action * self.max_acceleration

        # 2. æ›´æ–°é€Ÿåº¦ï¼ˆç§¯åˆ†ï¼‰
        new_velocity = self.agent_vel[agent_idx] + acceleration * self.dt

        # 3. åº”ç”¨é˜»å°¼
        new_velocity *= self.damping_factor

        # 4. åº”ç”¨è¿æ¥æ€§é€Ÿåº¦é™åˆ¶
        current_speed = np.linalg.norm(new_velocity)

        if current_speed > speed_limit:
            # é€Ÿåº¦è¶…é™ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
            if current_speed > 0:
                scale_factor = speed_limit / current_speed
                actual_velocity = new_velocity * scale_factor
            else:
                actual_velocity = new_velocity

            # è®°å½•è¿è§„
            speed_violations[agent_name] = {
                'desired_speed': current_speed,
                'limit': speed_limit,
                'violation': current_speed - speed_limit
            }
        else:
            actual_velocity = new_velocity
            speed_violations[agent_name] = {
                'desired_speed': current_speed,
                'limit': speed_limit,
                'violation': 0.0
            }

        # 5. åº”ç”¨æœ€å¤§é€Ÿåº¦é™åˆ¶
        final_speed = np.linalg.norm(actual_velocity)
        if final_speed > self.max_speed:
            actual_velocity = actual_velocity * (self.max_speed / final_speed)

        # 6. æ›´æ–°é€Ÿåº¦å’Œä½ç½®
        self.agent_vel[agent_idx] = actual_velocity
        self.agent_pos[agent_idx] += self.agent_vel[agent_idx] * self.dt

        # 7. è¾¹ç•Œå¤„ç†
        self.agent_pos[agent_idx] = np.clip(self.agent_pos[agent_idx],
                                          -self.world_size, self.world_size)

    def close(self):
        """å…³é—­ç¯å¢ƒ - ä¸åŸç‰ˆæœ¬ä¸€è‡´"""
        if self.screen is not None:
            pygame.quit()
            self.screen = None

    def calculate_coverage_complete(self):
        """è®¡ç®—å®Œæ•´è¦†ç›–ç‡ä¿¡æ¯ - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        # è®¡ç®—ç›®æ ‡ç‚¹æ˜¯å¦è¢«è¦†ç›–
        covered_flags = []
        for target in self.target_pos:
            covered = False
            for agent in self.agent_pos:
                distance = np.linalg.norm(target - agent)
                if distance <= self.coverage_radius:
                    covered = True
                    break
            covered_flags.append(covered)

        # è®¡ç®—è¦†ç›–ç‡
        covered_count = sum(covered_flags)
        total_targets = len(self.target_pos)
        coverage_rate = covered_count / total_targets if total_targets > 0 else 0

        # æ„å»ºé€šä¿¡é‚»æ¥çŸ©é˜µ
        num_agents = self.num_agents
        adjacency_matrix = np.zeros((num_agents, num_agents), dtype=bool)
        for i in range(num_agents):
            for j in range(i + 1, num_agents):
                distance = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                if distance <= self.communication_range:
                    adjacency_matrix[i, j] = True
                    adjacency_matrix[j, i] = True

        # DFS æ£€æŸ¥è¿é€šæ€§
        visited = [False] * num_agents

        def dfs(idx):
            visited[idx] = True
            for neighbor_idx, connected in enumerate(adjacency_matrix[idx]):
                if connected and not visited[neighbor_idx]:
                    dfs(neighbor_idx)

        dfs(0)
        fully_connected = all(visited)
        unconnected_count = visited.count(False)

        # æ›´æ–°æœ€å¤§è¦†ç›–ç‡
        if fully_connected:
            self.max_coverage_rate = max(self.max_coverage_rate, coverage_rate)

        return coverage_rate, fully_connected, self.max_coverage_rate, unconnected_count



    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        if self.render_mode is None:
            import gym
            gym.logger.warn(
                "Calling render without specifying render_mode."
            )
            return
        if self.screen is None:
            pygame.init()
            pygame.font.init()  # ç¡®ä¿å­—ä½“æ¨¡å—åˆå§‹åŒ–
            self.screen = pygame.display.set_mode((self.width, self.height))
            pygame.display.set_caption('UAV Topology')
            self.clock = pygame.time.Clock()

            # åˆå§‹åŒ–å­—ä½“
            try:
                # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                self.font = pygame.font.SysFont('arial', 24, bold=True)
                if self.font is None:
                    raise Exception("SysFont failed")
            except:
                try:
                    # å¤‡é€‰ï¼šä½¿ç”¨é»˜è®¤å­—ä½“
                    self.font = pygame.font.Font(None, 28)
                except:
                    # æœ€åå¤‡é€‰ï¼šåˆ›å»ºåŸºç¡€å­—ä½“
                    self.font = pygame.font.Font(pygame.font.get_default_font(), 24)
        self.draw()
        if self.render_mode == 'rgb_array':
            data = pygame.surfarray.array3d(self.screen)
            return np.transpose(data, (1, 0, 2))
        elif self.render_mode == 'human':
            pygame.display.flip()
            self.clock.tick(self.metadata['render_fps'])

    def draw(self):
        """ç»˜åˆ¶å‡½æ•° - ä¸åŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´"""
        fixed_cam = self.world_size
        self.screen.fill((255, 255, 255))

        def to_screen(pos):
            x, y = pos
            y = -y  # y è½´ç¿»è½¬
            sx = int((x / fixed_cam) * (self.width / 2) + self.width / 2)
            sy = int((y / fixed_cam) * (self.height / 2) + self.height / 2)
            return sx, sy

        # ç”»ç›®æ ‡ç‚¹
        for tpos in self.target_pos:
            sx, sy = to_screen(tpos)
            pygame.draw.circle(self.screen, (0, 255, 0), (sx, sy), 5)

        # å­˜å‚¨æ— äººæœºå±å¹•ä½ç½®ç”¨äºè¿çº¿
        screen_positions = []

        for i, apos in enumerate(self.agent_pos):
            sx, sy = to_screen(apos)
            screen_positions.append((sx, sy))

            # ç»˜åˆ¶æ— äººæœºå›¾åƒæˆ–é»˜è®¤å›¾å½¢
            if not hasattr(self, 'drone_image'):
                path = os.path.dirname(__file__)
                img = os.path.join(path, 'UAV.png')
                if os.path.exists(img):
                    self.drone_image = pygame.transform.scale(pygame.image.load(img), (30, 30))
                else:
                    self.drone_image = None
            if self.drone_image:
                rect = self.drone_image.get_rect(center=(sx, sy))
                self.screen.blit(self.drone_image, rect)
            else:
                # æ ¹æ®UAVæ˜¯å¦å¤±æ•ˆé€‰æ‹©é¢œè‰²
                if i in self.active_agents:
                    color = (0, 0, 255)  # è“è‰² - æ´»è·ƒUAV
                else:
                    color = (128, 128, 128)  # ç°è‰² - å¤±æ•ˆUAV
                pygame.draw.circle(self.screen, color, (sx, sy), 8)

            # ç»˜åˆ¶æ¢æµ‹åŠå¾„åœ†åœˆï¼ˆå®çº¿ï¼‰
            coverage_radius_px = int((self.coverage_radius / fixed_cam) * (self.width/2))
            pygame.draw.circle(self.screen, (0, 0, 255), (sx, sy), coverage_radius_px, 1)

            # ç»˜åˆ¶é€šä¿¡åŠå¾„åœ†åœˆï¼ˆè“è‰²è™šçº¿ï¼‰
            comm_radius_px = int((self.communication_range / fixed_cam) * (self.width/2))
            # åˆ›å»ºè™šçº¿æ•ˆæœ
            num_segments = 80  # è™šçº¿æ®µæ•°
            for j in range(num_segments):
                if j % 2 == 0:  # åªç”»å¶æ•°æ®µï¼Œå½¢æˆè™šçº¿
                    start_angle = 2 * np.pi * j / num_segments
                    end_angle = 2 * np.pi * (j + 1) / num_segments
                    # è®¡ç®—å¼§æ®µçš„èµ·ç‚¹å’Œç»ˆç‚¹
                    start_pos = (
                        sx + int(comm_radius_px * np.cos(start_angle)),
                        sy + int(comm_radius_px * np.sin(start_angle))
                    )
                    end_pos = (
                        sx + int(comm_radius_px * np.cos(end_angle)),
                        sy + int(comm_radius_px * np.sin(end_angle))
                    )
                    # ç”»è™šçº¿æ®µ
                    pygame.draw.line(self.screen, (70, 130, 180), start_pos, end_pos, 1)  # ä½¿ç”¨æµ…è“è‰²

        # ç”»çº¢çº¿ï¼šåªåœ¨æ´»è·ƒæ— äººæœºä¹‹é—´æ˜¾ç¤ºé€šä¿¡è¿æ¥
        for i in range(self.num_agents):
            for j in range(i + 1, self.num_agents):
                # åªæœ‰ä¸¤ä¸ªUAVéƒ½æ˜¯æ´»è·ƒçŠ¶æ€æ‰æ˜¾ç¤ºé€šä¿¡çº¿
                if i in self.active_agents and j in self.active_agents:
                    dist = np.linalg.norm(self.agent_pos[i] - self.agent_pos[j])
                    if dist <= self.communication_range:
                        pygame.draw.line(self.screen, (255, 0, 0),
                                        screen_positions[i], screen_positions[j], 1)

        # æ˜¾ç¤ºæ–‡å­—ä¿¡æ¯ï¼ˆä½¿ç”¨é¢„åˆå§‹åŒ–çš„å­—ä½“ï¼‰
        if self.font is None:
            # å¦‚æœå­—ä½“æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ç®€å•å¤‡é€‰
            font = pygame.font.Font(None, 24)
        else:
            font = self.font

        # æ˜¾ç¤ºå®éªŒç±»å‹ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…ç¼–ç é—®é¢˜ï¼‰
        experiment_text = f"Experiment: {self.experiment_type}"
        text_surface = font.render(experiment_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 10))

        # æ˜¾ç¤ºæ­¥æ•°
        step_text = f"Step: {self.curr_step}/{self.max_steps}"
        text_surface = font.render(step_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 40))

        # æ˜¾ç¤ºæ´»è·ƒUAVæ•°é‡
        uav_text = f"Active UAVs: {len(self.active_agents)}/{self.num_agents}"
        text_surface = font.render(uav_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 70))

        # æ˜¾ç¤ºè¦†ç›–ç‡ä¿¡æ¯
        coverage_rate, _, _, _ = self.calculate_coverage_complete()
        coverage_text = f"Coverage: {coverage_rate:.3f}"
        text_surface = font.render(coverage_text, True, (0, 0, 0))
        self.screen.blit(text_surface, (10, 100))

        # æ˜¾ç¤ºepisodeè®¡åˆ’ä¿¡æ¯
        if hasattr(self, 'episode_plan'):
            plan_type = self.episode_plan.get('type', 'unknown')
            trigger_step = self.episode_plan.get('trigger_step', None)
            executed = self.episode_plan.get('executed', False)

            plan_text = f"Plan: {plan_type}"
            if trigger_step:
                plan_text += f" (step {trigger_step})"
            if executed:
                plan_text += " [done]"

            text_surface = font.render(plan_text, True, (0, 0, 255))
            self.screen.blit(text_surface, (10, 130))



    # GATç›¸å…³æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_gat_parameters(self):
        """è·å–GATå‚æ•°"""
        return self.gat_model.parameters()

    def save_gat_model(self, path):
        """ä¿å­˜GATæ¨¡å‹"""
        torch.save(self.gat_model.state_dict(), path)

    def load_gat_model(self, path):
        """åŠ è½½GATæ¨¡å‹"""
        self.gat_model.load_state_dict(torch.load(path, map_location=self.device))

    # å…¼å®¹æ€§æ–¹æ³• - ä¿æŒåŸæœ‰æ¥å£
    def get_observation_space(self, agent):
        """è·å–è§‚å¯Ÿç©ºé—´"""
        return self.observation_spaces[agent]

    def get_action_space(self, agent):
        """è·å–åŠ¨ä½œç©ºé—´"""
        return self.action_spaces[agent]

    def fail_uav(self, uav_idx):
        """ä½¿UAVå¤±æ•ˆ - å…¼å®¹æ€§æ–¹æ³•"""
        if uav_idx in self.active_agents:
            self.active_agents.remove(uav_idx)
            # æ¸…é›¶å¤±æ•ˆUAVçš„é€Ÿåº¦å’Œå†å²åŠ¨ä½œ
            if uav_idx < len(self.agent_vel):
                self.agent_vel[uav_idx] = np.zeros(2, dtype=np.float32)
            if uav_idx < len(self.prev_actions):
                self.prev_actions[uav_idx] = np.zeros(2, dtype=np.float32)
            return True
        return False

    def add_uav(self):
        """æ·»åŠ UAV - å…¼å®¹æ€§æ–¹æ³•"""
        inactive_uavs = [i for i in range(self.num_agents) if i not in self.active_agents]
        if inactive_uavs:
            new_uav = inactive_uavs[0]
            self.active_agents.append(new_uav)
            self.agent_pos[new_uav] = np.random.uniform(-self.world_size, self.world_size, 2)
            self.agent_vel[new_uav] = np.zeros(2)  # æ–°UAVä»é™æ­¢å¼€å§‹
            # é‡ç½®æ–°æ¿€æ´»UAVçš„å†å²åŠ¨ä½œ
            if new_uav < len(self.prev_actions):
                self.prev_actions[new_uav] = np.zeros(2, dtype=np.float32)
            return new_uav
        return None

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.screen is not None:
            pygame.display.quit()
            pygame.quit()
            self.screen = None
            self.clock = None
